dnn_hidden_units : 100
learning_rate : 0.002
max_steps : 1500
batch_size : 200
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
Step 1 of project Take Over the World: distinguish between cats and dogs
ind 0 epoc 0
dnn_hidden_units : 50,10
learning_rate : 0.0006050711394226951
max_steps : 3191
batch_size : 794
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0006050711394226951
step:0
train performance: acc = 0.10453400503778337, loss = 2.302590053153385
test performance: acc = 0.0979, loss = 2.302587986861215
learning rate: 0.0006050711394226951
step:100
train performance: acc = 0.08816120906801007, loss = 2.3025856864159624
test performance: acc = 0.085, loss = 2.3025867469160075
learning rate: 0.0006050711394226951
step:200
train performance: acc = 0.12342569269521411, loss = 2.3025826129455833
test performance: acc = 0.1207, loss = 2.30258547583383
learning rate: 0.0006050711394226951
step:300
train performance: acc = 0.09319899244332494, loss = 2.302585338390685
test performance: acc = 0.1033, loss = 2.302584127422065
learning rate: 0.0006050711394226951
step:400
train performance: acc = 0.11460957178841309, loss = 2.302580745764517
test performance: acc = 0.1083, loss = 2.3025826692148077
learning rate: 0.0006050711394226951
step:500
train performance: acc = 0.11335012594458438, loss = 2.302581845782742
test performance: acc = 0.1209, loss = 2.302581060878363
learning rate: 0.0006050711394226951
step:600
train performance: acc = 0.11838790931989925, loss = 2.302580945188139
test performance: acc = 0.1405, loss = 2.3025792883573617
learning rate: 0.0006050711394226951
step:700
train performance: acc = 0.1486146095717884, loss = 2.302575879242568
test performance: acc = 0.1267, loss = 2.3025773247106094
learning rate: 0.0006050711394226951
step:800
train performance: acc = 0.12342569269521411, loss = 2.302574172496372
test performance: acc = 0.1209, loss = 2.3025751643577705
learning rate: 0.0006050711394226951
step:900
train performance: acc = 0.12594458438287154, loss = 2.302571045123888
test performance: acc = 0.1077, loss = 2.3025727612975846
learning rate: 0.0006050711394226951
step:1000
train performance: acc = 0.10831234256926953, loss = 2.302571271142369
test performance: acc = 0.1103, loss = 2.3025700678247265
learning rate: 0.0006050711394226951
step:1100
train performance: acc = 0.1624685138539043, loss = 2.302565300469081
test performance: acc = 0.1472, loss = 2.302567006313763
learning rate: 0.0006050711394226951
step:1200
train performance: acc = 0.1385390428211587, loss = 2.3025637252638322
test performance: acc = 0.1314, loss = 2.3025634739046716
learning rate: 0.0006050711394226951
step:1300
train performance: acc = 0.13224181360201512, loss = 2.3025612334171686
test performance: acc = 0.1335, loss = 2.302559358907932
learning rate: 0.0006050711394226951
step:1400
train performance: acc = 0.11083123425692695, loss = 2.3025624003518046
test performance: acc = 0.1221, loss = 2.3025544922726193
learning rate: 0.0006050711394226951
step:1500
train performance: acc = 0.13602015113350127, loss = 2.3025526012849156
test performance: acc = 0.1253, loss = 2.3025486785749623
learning rate: 0.0006050711394226951
step:1600
train performance: acc = 0.14231738035264482, loss = 2.302542829095447
test performance: acc = 0.1284, loss = 2.3025416118419804
learning rate: 0.0006050711394226951
step:1700
train performance: acc = 0.16498740554156172, loss = 2.3025350224301233
test performance: acc = 0.1544, loss = 2.302532974195488
learning rate: 0.0006050711394226951
step:1800
train performance: acc = 0.16498740554156172, loss = 2.302519111414672
test performance: acc = 0.1518, loss = 2.3025221314734803
learning rate: 0.0006050711394226951
step:1900
train performance: acc = 0.16120906801007556, loss = 2.302504307679655
test performance: acc = 0.1466, loss = 2.3025081901447337
learning rate: 0.0006050711394226951
step:2000
train performance: acc = 0.15869017632241814, loss = 2.302488183365939
test performance: acc = 0.1467, loss = 2.3024904274487685
learning rate: 0.0006050711394226951
step:2100
train performance: acc = 0.1624685138539043, loss = 2.3024683974752214
test performance: acc = 0.145, loss = 2.302466773533002
learning rate: 0.0006050711394226951
step:2200
train performance: acc = 0.13350125944584382, loss = 2.302447432792864
test performance: acc = 0.1372, loss = 2.302434537822694
learning rate: 0.0006050711394226951
step:2300
train performance: acc = 0.12090680100755667, loss = 2.3024023846247488
test performance: acc = 0.1426, loss = 2.302389671907105
learning rate: 0.0006050711394226951
step:2400
train performance: acc = 0.16624685138539042, loss = 2.302300972791441
test performance: acc = 0.1538, loss = 2.3023251591406253
learning rate: 0.0006050711394226951
step:2500
train performance: acc = 0.13602015113350127, loss = 2.3022445031215577
test performance: acc = 0.1304, loss = 2.302226831988003
learning rate: 0.0006050711394226951
step:2600
train performance: acc = 0.1397984886649874, loss = 2.302096544797967
test performance: acc = 0.1388, loss = 2.302075863796156
learning rate: 0.0006050711394226951
step:2700
train performance: acc = 0.15239294710327456, loss = 2.301829764384189
test performance: acc = 0.1412, loss = 2.3018227663189235
learning rate: 0.0006050711394226951
step:2800
train performance: acc = 0.1712846347607053, loss = 2.301346850686623
test performance: acc = 0.1431, loss = 2.301366271589117
learning rate: 0.0006050711394226951
step:2900
train performance: acc = 0.15239294710327456, loss = 2.300294529511129
test performance: acc = 0.1387, loss = 2.3004583767583138
learning rate: 0.0006050711394226951
step:3000
train performance: acc = 0.13602015113350127, loss = 2.298419115226667
test performance: acc = 0.1484, loss = 2.298386936007068
learning rate: 0.0006050711394226951
step:3100
train performance: acc = 0.172544080604534, loss = 2.2917300309811512
test performance: acc = 0.1455, loss = 2.292760235656249
learning rate: 0.0006050711394226951
step:3190
train performance: acc = 0.1574307304785894, loss = 2.279247073514809
test performance: acc = 0.1533, loss = 2.2775948720065458
saving results in folder...
saving model in folder
ind 1 epoc 0
dnn_hidden_units : 200
learning_rate : 0.0009578617499304639
max_steps : 4274
batch_size : 191
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0009578617499304639
step:0
train performance: acc = 0.06806282722513089, loss = 2.3032495263908808
test performance: acc = 0.1449, loss = 2.2976526565598543
learning rate: 0.0009578617499304639
step:100
train performance: acc = 0.3403141361256545, loss = 1.7587822585143027
test performance: acc = 0.3788, loss = 1.7643915604366
learning rate: 0.0009578617499304639
step:200
train performance: acc = 0.4397905759162304, loss = 1.7365818733910834
test performance: acc = 0.4213, loss = 1.6490744780043767
learning rate: 0.0009578617499304639
step:300
train performance: acc = 0.41361256544502617, loss = 1.6052973981046474
test performance: acc = 0.4452, loss = 1.5813992013784672
learning rate: 0.0009578617499304639
step:400
train performance: acc = 0.5026178010471204, loss = 1.4534121495434813
test performance: acc = 0.4614, loss = 1.5290185939756038
learning rate: 0.0009578617499304639
step:500
train performance: acc = 0.4712041884816754, loss = 1.4793020792052745
test performance: acc = 0.469, loss = 1.5012267678277742
learning rate: 0.0009578617499304639
step:600
train performance: acc = 0.4973821989528796, loss = 1.4228816379486053
test performance: acc = 0.4834, loss = 1.467583335762053
learning rate: 0.0009578617499304639
step:700
train performance: acc = 0.49214659685863876, loss = 1.3534013732698564
test performance: acc = 0.4855, loss = 1.456777502299752
learning rate: 0.0009578617499304639
step:800
train performance: acc = 0.5968586387434555, loss = 1.25212944581863
test performance: acc = 0.495, loss = 1.4394451801997457
learning rate: 0.0009578617499304639
step:900
train performance: acc = 0.518324607329843, loss = 1.357678487104572
test performance: acc = 0.4953, loss = 1.4499216714243388
learning rate: 0.0009578617499304639
step:1000
train performance: acc = 0.518324607329843, loss = 1.3829905737619983
test performance: acc = 0.5063, loss = 1.4121844786495197
learning rate: 0.0009578617499304639
step:1100
train performance: acc = 0.5706806282722513, loss = 1.2098404929141717
test performance: acc = 0.4965, loss = 1.4460551425721921
learning rate: 0.0009578617499304639
step:1200
train performance: acc = 0.5340314136125655, loss = 1.3708672064631446
test performance: acc = 0.4901, loss = 1.442193946795072
learning rate: 0.0009578617499304639
step:1300
train performance: acc = 0.450261780104712, loss = 1.4544661114595943
test performance: acc = 0.4944, loss = 1.4449940268455301
learning rate: 0.0009578617499304639
step:1400
train performance: acc = 0.5602094240837696, loss = 1.1727286584109236
test performance: acc = 0.4894, loss = 1.4744214640999838
learning rate: 0.0009578617499304639
step:1500
train performance: acc = 0.5392670157068062, loss = 1.2520529994585483
test performance: acc = 0.5072, loss = 1.4173256771387568
learning rate: 0.0009578617499304639
step:1600
train performance: acc = 0.6387434554973822, loss = 1.122847521877464
test performance: acc = 0.511, loss = 1.3981577727431336
learning rate: 0.0009578617499304639
step:1700
train performance: acc = 0.5497382198952879, loss = 1.361317423311669
test performance: acc = 0.5154, loss = 1.383910725712205
learning rate: 0.0009578617499304639
step:1800
train performance: acc = 0.5759162303664922, loss = 1.3493174719340408
test performance: acc = 0.5139, loss = 1.3873966355459804
learning rate: 0.0009578617499304639
step:1900
train performance: acc = 0.4712041884816754, loss = 1.4242811996261608
test performance: acc = 0.5152, loss = 1.4060130653360814
learning rate: 0.0009578617499304639
step:2000
train performance: acc = 0.5654450261780105, loss = 1.2472366461929407
test performance: acc = 0.4958, loss = 1.4832855702584777
Early Stop
saving results in folder...
saving model in folder
ind 2 epoc 0
dnn_hidden_units : 200
learning_rate : 0.0003986025661559687
max_steps : 1932
batch_size : 238
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0003986025661559687
step:0
train performance: acc = 0.07563025210084033, loss = 2.3021216285014243
test performance: acc = 0.1164, loss = 2.301915753567231
learning rate: 0.0003986025661559687
step:100
train performance: acc = 0.3277310924369748, loss = 1.9059195129060647
test performance: acc = 0.3155, loss = 1.9269842579835665
learning rate: 0.0003986025661559687
step:200
train performance: acc = 0.4117647058823529, loss = 1.7424636965133484
test performance: acc = 0.3693, loss = 1.7869369781100501
learning rate: 0.0003986025661559687
step:300
train performance: acc = 0.40756302521008403, loss = 1.5788600126424976
test performance: acc = 0.4023, loss = 1.7029982386236628
learning rate: 0.0003986025661559687
step:400
train performance: acc = 0.36554621848739494, loss = 1.7596834892567819
test performance: acc = 0.4194, loss = 1.6487417186904765
learning rate: 0.0003986025661559687
step:500
train performance: acc = 0.4117647058823529, loss = 1.6040943338688438
test performance: acc = 0.4381, loss = 1.6085726549896242
learning rate: 0.0003986025661559687
step:600
train performance: acc = 0.4957983193277311, loss = 1.5954975320559919
test performance: acc = 0.4494, loss = 1.5788792470967965
learning rate: 0.0003986025661559687
step:700
train performance: acc = 0.40756302521008403, loss = 1.6140913680950413
test performance: acc = 0.4574, loss = 1.5493751736433403
learning rate: 0.0003986025661559687
step:800
train performance: acc = 0.4789915966386555, loss = 1.559364643111832
test performance: acc = 0.4679, loss = 1.5260753153543722
learning rate: 0.0003986025661559687
step:900
train performance: acc = 0.47058823529411764, loss = 1.3989271199636795
test performance: acc = 0.4716, loss = 1.5077254090617649
learning rate: 0.0003986025661559687
step:1000
train performance: acc = 0.5126050420168067, loss = 1.4314799424166078
test performance: acc = 0.4771, loss = 1.488843847119104
learning rate: 0.0003986025661559687
step:1100
train performance: acc = 0.5336134453781513, loss = 1.4584421009180883
test performance: acc = 0.4787, loss = 1.479820335977993
learning rate: 0.0003986025661559687
step:1200
train performance: acc = 0.47478991596638653, loss = 1.4671127995085769
test performance: acc = 0.4841, loss = 1.4651422059988715
learning rate: 0.0003986025661559687
step:1300
train performance: acc = 0.5042016806722689, loss = 1.393262445639187
test performance: acc = 0.4921, loss = 1.4509860501191552
learning rate: 0.0003986025661559687
step:1400
train performance: acc = 0.5042016806722689, loss = 1.4118312179447063
test performance: acc = 0.4984, loss = 1.4349504453245276
learning rate: 0.0003986025661559687
step:1500
train performance: acc = 0.5, loss = 1.3436190028770472
test performance: acc = 0.4988, loss = 1.4289338675874692
learning rate: 0.0003986025661559687
step:1600
train performance: acc = 0.4957983193277311, loss = 1.3521044638499495
test performance: acc = 0.4999, loss = 1.4250558155873359
learning rate: 0.0003986025661559687
step:1700
train performance: acc = 0.5252100840336135, loss = 1.4034324773737814
test performance: acc = 0.506, loss = 1.41137997824364
learning rate: 0.0003986025661559687
step:1800
train performance: acc = 0.5126050420168067, loss = 1.3741039873068916
test performance: acc = 0.5034, loss = 1.4091185079543196
learning rate: 0.0003986025661559687
step:1900
train performance: acc = 0.6050420168067226, loss = 1.1781988067257645
test performance: acc = 0.5077, loss = 1.4026430138137584
learning rate: 0.0003986025661559687
step:1931
train performance: acc = 0.6134453781512605, loss = 1.197481936879843
test performance: acc = 0.5113, loss = 1.3964410132787544
saving results in folder...
saving model in folder
ind 3 epoc 0
dnn_hidden_units : 50,10
learning_rate : 0.0011359647620543263
max_steps : 4831
batch_size : 236
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0011359647620543263
step:0
train performance: acc = 0.1059322033898305, loss = 2.30258912315686
test performance: acc = 0.0914, loss = 2.302587978634831
learning rate: 0.0011359647620543263
step:100
train performance: acc = 0.0847457627118644, loss = 2.302603378922597
test performance: acc = 0.1021, loss = 2.3025855967838953
learning rate: 0.0011359647620543263
step:200
train performance: acc = 0.1059322033898305, loss = 2.3025834106574563
test performance: acc = 0.1073, loss = 2.3025830294161453
learning rate: 0.0011359647620543263
step:300
train performance: acc = 0.0635593220338983, loss = 2.3025928173460466
test performance: acc = 0.1, loss = 2.3025798864394993
learning rate: 0.0011359647620543263
step:400
train performance: acc = 0.1059322033898305, loss = 2.3025826799436273
test performance: acc = 0.1189, loss = 2.3025761957306714
learning rate: 0.0011359647620543263
step:500
train performance: acc = 0.11440677966101695, loss = 2.302563816271948
test performance: acc = 0.0996, loss = 2.302571707859697
learning rate: 0.0011359647620543263
step:600
train performance: acc = 0.09745762711864407, loss = 2.3025840184035715
test performance: acc = 0.1029, loss = 2.302565982474881
learning rate: 0.0011359647620543263
step:700
train performance: acc = 0.13135593220338984, loss = 2.3025499710182977
test performance: acc = 0.1015, loss = 2.3025585918690883
learning rate: 0.0011359647620543263
step:800
train performance: acc = 0.07203389830508475, loss = 2.302570454302374
test performance: acc = 0.108, loss = 2.3025483595134335
learning rate: 0.0011359647620543263
step:900
train performance: acc = 0.1440677966101695, loss = 2.302531432902723
test performance: acc = 0.1476, loss = 2.3025337860619093
learning rate: 0.0011359647620543263
step:1000
train performance: acc = 0.1440677966101695, loss = 2.302528605985011
test performance: acc = 0.128, loss = 2.3025118635023305
learning rate: 0.0011359647620543263
step:1100
train performance: acc = 0.1694915254237288, loss = 2.302477869423637
test performance: acc = 0.1503, loss = 2.302475436704187
learning rate: 0.0011359647620543263
step:1200
train performance: acc = 0.1652542372881356, loss = 2.3024100968377783
test performance: acc = 0.1372, loss = 2.302411562831454
learning rate: 0.0011359647620543263
step:1300
train performance: acc = 0.13983050847457626, loss = 2.3022335164444088
test performance: acc = 0.1181, loss = 2.3022886063648222
learning rate: 0.0011359647620543263
step:1400
train performance: acc = 0.1652542372881356, loss = 2.3019027486937134
test performance: acc = 0.1486, loss = 2.3020246951819403
learning rate: 0.0011359647620543263
step:1500
train performance: acc = 0.13983050847457626, loss = 2.3013995510438217
test performance: acc = 0.1358, loss = 2.3012484804509166
learning rate: 0.0011359647620543263
step:1600
train performance: acc = 0.1271186440677966, loss = 2.2981966784379564
test performance: acc = 0.1518, loss = 2.2980961369696327
learning rate: 0.0011359647620543263
step:1700
train performance: acc = 0.11864406779661017, loss = 2.275776651425206
test performance: acc = 0.1562, loss = 2.2772380617111483
learning rate: 0.0011359647620543263
step:1800
train performance: acc = 0.1271186440677966, loss = 2.250015137088399
test performance: acc = 0.1488, loss = 2.2145416596553624
learning rate: 0.0011359647620543263
step:1900
train performance: acc = 0.1864406779661017, loss = 2.142159619526756
test performance: acc = 0.1744, loss = 2.1858887759341052
learning rate: 0.0011359647620543263
step:2000
train performance: acc = 0.1906779661016949, loss = 2.1891897893657086
test performance: acc = 0.1752, loss = 2.175587312521986
learning rate: 0.0011359647620543263
step:2100
train performance: acc = 0.15677966101694915, loss = 2.1857608923572838
test performance: acc = 0.1787, loss = 2.1699549213680096
learning rate: 0.0011359647620543263
step:2200
train performance: acc = 0.19915254237288135, loss = 2.1631531777850874
test performance: acc = 0.1795, loss = 2.1668796936167016
learning rate: 0.0011359647620543263
step:2300
train performance: acc = 0.1906779661016949, loss = 2.1352147352087467
test performance: acc = 0.1811, loss = 2.163980452278624
learning rate: 0.0011359647620543263
step:2400
train performance: acc = 0.1864406779661017, loss = 2.1670273599537526
test performance: acc = 0.1814, loss = 2.1615529448079904
learning rate: 0.0011359647620543263
step:2500
train performance: acc = 0.1694915254237288, loss = 2.132260137203262
test performance: acc = 0.1812, loss = 2.158328460928177
learning rate: 0.0011359647620543263
step:2600
train performance: acc = 0.1652542372881356, loss = 2.1905969392760754
test performance: acc = 0.1818, loss = 2.155502162073946
learning rate: 0.0011359647620543263
step:2700
train performance: acc = 0.1864406779661017, loss = 2.1134243931145082
test performance: acc = 0.1834, loss = 2.151653842772682
learning rate: 0.0011359647620543263
step:2800
train performance: acc = 0.15677966101694915, loss = 2.127299061835486
test performance: acc = 0.1835, loss = 2.1455045492219216
learning rate: 0.0011359647620543263
step:2900
train performance: acc = 0.1906779661016949, loss = 2.1317464892272255
test performance: acc = 0.1863, loss = 2.128651891493184
learning rate: 0.0011359647620543263
step:3000
train performance: acc = 0.2330508474576271, loss = 2.04161350645059
test performance: acc = 0.1915, loss = 2.0845737430471334
learning rate: 0.0011359647620543263
step:3100
train performance: acc = 0.16101694915254236, loss = 2.052227432012142
test performance: acc = 0.1902, loss = 2.0578944897820772
learning rate: 0.0011359647620543263
step:3200
train performance: acc = 0.1864406779661017, loss = 2.0294272911313627
test performance: acc = 0.1962, loss = 2.040560013314421
learning rate: 0.0011359647620543263
step:3300
train performance: acc = 0.19915254237288135, loss = 2.03764535722081
test performance: acc = 0.2017, loss = 2.024253531481387
learning rate: 0.0011359647620543263
step:3400
train performance: acc = 0.2457627118644068, loss = 2.043577912170936
test performance: acc = 0.2054, loss = 2.0102482617281483
learning rate: 0.0011359647620543263
step:3500
train performance: acc = 0.2457627118644068, loss = 2.0193715708207454
test performance: acc = 0.2145, loss = 1.9987870676606883
learning rate: 0.0011359647620543263
step:3600
train performance: acc = 0.24152542372881355, loss = 2.0107669093106715
test performance: acc = 0.2121, loss = 1.985728691496173
learning rate: 0.0011359647620543263
step:3700
train performance: acc = 0.2669491525423729, loss = 1.8510477238143404
test performance: acc = 0.2257, loss = 1.9729323370453444
learning rate: 0.0011359647620543263
step:3800
train performance: acc = 0.2669491525423729, loss = 1.9058979866325387
test performance: acc = 0.2533, loss = 1.9487968150547756
learning rate: 0.0011359647620543263
step:3900
train performance: acc = 0.2330508474576271, loss = 1.8793611736233382
test performance: acc = 0.2558, loss = 1.9168615973744527
learning rate: 0.0011359647620543263
step:4000
train performance: acc = 0.2754237288135593, loss = 1.9832262774119434
test performance: acc = 0.2693, loss = 1.8902207695509885
learning rate: 0.0011359647620543263
step:4100
train performance: acc = 0.3177966101694915, loss = 1.8633304958639119
test performance: acc = 0.287, loss = 1.8701152654806679
learning rate: 0.0011359647620543263
step:4200
train performance: acc = 0.2754237288135593, loss = 1.8890451152035308
test performance: acc = 0.3015, loss = 1.8515876663426591
learning rate: 0.0011359647620543263
step:4300
train performance: acc = 0.3135593220338983, loss = 1.8113707677669564
test performance: acc = 0.3106, loss = 1.8337742399697803
learning rate: 0.0011359647620543263
step:4400
train performance: acc = 0.288135593220339, loss = 1.8439193851959514
test performance: acc = 0.3133, loss = 1.8184447500816316
learning rate: 0.0011359647620543263
step:4500
train performance: acc = 0.3559322033898305, loss = 1.8154805454119973
test performance: acc = 0.3273, loss = 1.79801753376371
learning rate: 0.0011359647620543263
step:4600
train performance: acc = 0.3389830508474576, loss = 1.813263175745647
test performance: acc = 0.3404, loss = 1.779150154500538
learning rate: 0.0011359647620543263
step:4700
train performance: acc = 0.3644067796610169, loss = 1.7403812751811
test performance: acc = 0.3443, loss = 1.76464034426555
learning rate: 0.0011359647620543263
step:4800
train performance: acc = 0.4152542372881356, loss = 1.6845305967417905
test performance: acc = 0.3547, loss = 1.7492198797871714
learning rate: 0.0011359647620543263
step:4830
train performance: acc = 0.3093220338983051, loss = 1.7483350293548805
test performance: acc = 0.3529, loss = 1.7446082790948065
saving results in folder...
saving model in folder
ind 4 epoc 0
dnn_hidden_units : 300100,10
learning_rate : 9.374542727072346e-05
max_steps : 2281
batch_size : 811
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 9.374542727072346e-05
step:0
train performance: acc = 0.08754623921085081, loss = 2.3024344807584347
test performance: acc = 0.0973, loss = 2.3024066359278885
slurmstepd: error: *** JOB 2105893 ON r30n6 CANCELLED AT 2019-04-12T08:45:12 ***
