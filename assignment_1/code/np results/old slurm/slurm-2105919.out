dnn_hidden_units : 100
learning_rate : 0.002
max_steps : 1500
batch_size : 200
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
Step 1 of project Take Over the World: distinguish between cats and dogs
ind 0 epoc 0
dnn_hidden_units : 200
learning_rate : 0.0017420717318036194
max_steps : 3907
batch_size : 108
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0017420717318036194
step:0
train performance: acc = 0.06481481481481481, loss = 2.30527890410177
test performance: acc = 0.1512, loss = 2.2930478041020543
learning rate: 0.0017420717318036194
step:100
train performance: acc = 0.37037037037037035, loss = 1.8080934281612502
test performance: acc = 0.3816, loss = 1.718463871538735
learning rate: 0.0017420717318036194
step:200
train performance: acc = 0.46296296296296297, loss = 1.5371125563869579
test performance: acc = 0.4031, loss = 1.6662162978895172
learning rate: 0.0017420717318036194
step:300
train performance: acc = 0.37037037037037035, loss = 1.6968979840410765
test performance: acc = 0.4041, loss = 1.6388979605746858
learning rate: 0.0017420717318036194
step:400
train performance: acc = 0.3148148148148148, loss = 1.8491783803572206
test performance: acc = 0.4058, loss = 1.7004992501462195
learning rate: 0.0017420717318036194
step:500
train performance: acc = 0.5370370370370371, loss = 1.3868961478478141
test performance: acc = 0.447, loss = 1.5676572081028257
learning rate: 0.0017420717318036194
step:600
train performance: acc = 0.4537037037037037, loss = 1.5696504797614534
test performance: acc = 0.4561, loss = 1.5342248842722988
learning rate: 0.0017420717318036194
step:700
train performance: acc = 0.4166666666666667, loss = 1.5848194128596265
test performance: acc = 0.4472, loss = 1.5683313614602172
learning rate: 0.0017420717318036194
step:800
train performance: acc = 0.39814814814814814, loss = 1.6427488301167739
test performance: acc = 0.4484, loss = 1.5948918356220507
learning rate: 0.0017420717318036194
step:900
train performance: acc = 0.49074074074074076, loss = 1.570617669207162
test performance: acc = 0.4198, loss = 1.7045659628924434
learning rate: 0.0017420717318036194
step:1000
train performance: acc = 0.42592592592592593, loss = 1.4743418104940607
test performance: acc = 0.458, loss = 1.6009290108165948
learning rate: 0.0017420717318036194
step:1100
train performance: acc = 0.4074074074074074, loss = 1.671749207795367
test performance: acc = 0.4496, loss = 1.5629241528429356
learning rate: 0.0017420717318036194
step:1200
train performance: acc = 0.4722222222222222, loss = 1.5214034455815044
test performance: acc = 0.4214, loss = 1.6846447959257476
learning rate: 0.0017420717318036194
step:1300
train performance: acc = 0.4351851851851852, loss = 1.6890242258833272
test performance: acc = 0.4315, loss = 1.6646177115469734
learning rate: 0.0017420717318036194
step:1400
train performance: acc = 0.4722222222222222, loss = 1.48411871346122
test performance: acc = 0.4557, loss = 1.5651912520122355
Early Stop
saving results in folder...
saving model in folder
ind 1 epoc 0
dnn_hidden_units : 300
learning_rate : 0.0019543871118370758
max_steps : 4356
batch_size : 425
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0019543871118370758
step:0
train performance: acc = 0.08705882352941176, loss = 2.3075178477746245
test performance: acc = 0.1948, loss = 2.2795879967580253
learning rate: 0.0019543871118370758
step:100
train performance: acc = 0.3835294117647059, loss = 1.7524820769647957
test performance: acc = 0.4093, loss = 1.6676595511249264
learning rate: 0.0019543871118370758
step:200
train performance: acc = 0.4117647058823529, loss = 1.60019934621788
test performance: acc = 0.4554, loss = 1.548692347329499
learning rate: 0.0019543871118370758
step:300
train performance: acc = 0.4752941176470588, loss = 1.4521878564687578
test performance: acc = 0.4573, loss = 1.536205937444477
learning rate: 0.0019543871118370758
step:400
train performance: acc = 0.4894117647058824, loss = 1.4956889841391474
test performance: acc = 0.48, loss = 1.4744649601224502
learning rate: 0.0019543871118370758
step:500
train performance: acc = 0.5129411764705882, loss = 1.3819374889577658
test performance: acc = 0.4743, loss = 1.4873664402665754
learning rate: 0.0019543871118370758
step:600
train performance: acc = 0.56, loss = 1.2689551095433553
test performance: acc = 0.5005, loss = 1.4319984387514522
learning rate: 0.0019543871118370758
step:700
train performance: acc = 0.48, loss = 1.5670533978798606
test performance: acc = 0.4741, loss = 1.5198791048144402
learning rate: 0.0019543871118370758
step:800
train performance: acc = 0.47294117647058825, loss = 1.483512471859566
test performance: acc = 0.4296, loss = 1.696920325072873
learning rate: 0.0019543871118370758
step:900
train performance: acc = 0.5505882352941176, loss = 1.2962433954802839
test performance: acc = 0.4899, loss = 1.474138493198949
learning rate: 0.0019543871118370758
step:1000
train performance: acc = 0.5505882352941176, loss = 1.3575997054113123
test performance: acc = 0.4663, loss = 1.6027824612935568
learning rate: 0.0019543871118370758
step:1100
train performance: acc = 0.5317647058823529, loss = 1.3368379686467953
test performance: acc = 0.4813, loss = 1.5067990913894602
learning rate: 0.0019543871118370758
step:1200
train performance: acc = 0.5176470588235295, loss = 1.4249521020675726
test performance: acc = 0.465, loss = 1.623920998014373
learning rate: 0.0019543871118370758
step:1300
train performance: acc = 0.6, loss = 1.194247067684263
test performance: acc = 0.466, loss = 1.5618300114571595
learning rate: 0.0019543871118370758
step:1400
train performance: acc = 0.5388235294117647, loss = 1.304422284085289
test performance: acc = 0.4932, loss = 1.497232425986493
learning rate: 0.0019543871118370758
step:1500
train performance: acc = 0.5411764705882353, loss = 1.3213839012792634
test performance: acc = 0.4814, loss = 1.5250928501940348
learning rate: 0.0019543871118370758
step:1600
train performance: acc = 0.5105882352941177, loss = 1.4183017504848945
test performance: acc = 0.4707, loss = 1.6544514086188384
learning rate: 0.0019543871118370758
step:1700
train performance: acc = 0.6376470588235295, loss = 1.086259301595327
test performance: acc = 0.5042, loss = 1.4477231361414125
learning rate: 0.0019543871118370758
step:1800
train performance: acc = 0.5623529411764706, loss = 1.2177457629310582
test performance: acc = 0.4702, loss = 1.6127591952222347
learning rate: 0.0019543871118370758
step:1900
train performance: acc = 0.5788235294117647, loss = 1.2656822125611555
test performance: acc = 0.4913, loss = 1.5859978577017566
learning rate: 0.0019543871118370758
step:2000
train performance: acc = 0.6494117647058824, loss = 1.0060890285653128
test performance: acc = 0.4877, loss = 1.5270029437192396
learning rate: 0.0019543871118370758
step:2100
train performance: acc = 0.5788235294117647, loss = 1.2704068283505316
test performance: acc = 0.4962, loss = 1.5197300680584573
learning rate: 0.0019543871118370758
step:2200
train performance: acc = 0.5435294117647059, loss = 1.4165174826115465
test performance: acc = 0.4819, loss = 1.6078026242887757
learning rate: 0.0019543871118370758
step:2300
train performance: acc = 0.5411764705882353, loss = 1.4171129834775251
test performance: acc = 0.4742, loss = 1.7258267641901264
Early Stop
saving results in folder...
saving model in folder
ind 2 epoc 0
dnn_hidden_units : 50,10
learning_rate : 0.001985352700686262
max_steps : 1873
batch_size : 236
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.001985352700686262
step:0
train performance: acc = 0.1059322033898305, loss = 2.30258912315686
test performance: acc = 0.0943, loss = 2.302587962128526
learning rate: 0.001985352700686262
step:100
train performance: acc = 0.09745762711864407, loss = 2.302614405820423
test performance: acc = 0.1006, loss = 2.302583668212358
learning rate: 0.001985352700686262
step:200
train performance: acc = 0.1059322033898305, loss = 2.3025772766677917
test performance: acc = 0.102, loss = 2.302578201757154
learning rate: 0.001985352700686262
step:300
train performance: acc = 0.0635593220338983, loss = 2.3025909824105395
test performance: acc = 0.1, loss = 2.3025703654349816
learning rate: 0.001985352700686262
step:400
train performance: acc = 0.11016949152542373, loss = 2.302569067666169
test performance: acc = 0.1277, loss = 2.3025586150552275
learning rate: 0.001985352700686262
step:500
train performance: acc = 0.11016949152542373, loss = 2.30252659702918
test performance: acc = 0.1033, loss = 2.3025382859892036
learning rate: 0.001985352700686262
step:600
train performance: acc = 0.1059322033898305, loss = 2.3025401411663746
test performance: acc = 0.1055, loss = 2.302495736123286
learning rate: 0.001985352700686262
step:700
train performance: acc = 0.1271186440677966, loss = 2.3023740827567662
test performance: acc = 0.1004, loss = 2.3023900945891445
learning rate: 0.001985352700686262
step:800
train performance: acc = 0.08050847457627118, loss = 2.3020831633079557
test performance: acc = 0.124, loss = 2.302020633205619
learning rate: 0.001985352700686262
step:900
train performance: acc = 0.15254237288135594, loss = 2.300273993002519
test performance: acc = 0.1409, loss = 2.299606210624086
learning rate: 0.001985352700686262
step:1000
train performance: acc = 0.13559322033898305, loss = 2.2471260340762864
test performance: acc = 0.1465, loss = 2.246775235083381
learning rate: 0.001985352700686262
step:1100
train performance: acc = 0.2033898305084746, loss = 2.1699408519607255
test performance: acc = 0.1746, loss = 2.184903598236646
learning rate: 0.001985352700686262
step:1200
train performance: acc = 0.1652542372881356, loss = 2.1537197145279676
test performance: acc = 0.18, loss = 2.1712345574970326
learning rate: 0.001985352700686262
step:1300
train performance: acc = 0.17372881355932204, loss = 2.11907022057714
test performance: acc = 0.1805, loss = 2.1649499843355713
learning rate: 0.001985352700686262
step:1400
train performance: acc = 0.19915254237288135, loss = 2.1447870719337137
test performance: acc = 0.1824, loss = 2.16039906484196
learning rate: 0.001985352700686262
step:1500
train performance: acc = 0.17796610169491525, loss = 2.156957575546036
test performance: acc = 0.1801, loss = 2.155418999284173
learning rate: 0.001985352700686262
step:1600
train performance: acc = 0.16101694915254236, loss = 2.1652232527980577
test performance: acc = 0.1827, loss = 2.1463652483681313
learning rate: 0.001985352700686262
step:1700
train performance: acc = 0.16101694915254236, loss = 2.121869703690206
test performance: acc = 0.1923, loss = 2.097195834186759
learning rate: 0.001985352700686262
step:1800
train performance: acc = 0.18220338983050846, loss = 2.0468741993397157
test performance: acc = 0.1928, loss = 2.0513362917548084
learning rate: 0.001985352700686262
step:1872
train performance: acc = 0.211864406779661, loss = 2.0220597640204865
test performance: acc = 0.1989, loss = 2.0323160200102173
saving results in folder...
saving model in folder
ind 3 epoc 0
dnn_hidden_units : 200
learning_rate : 0.00015670315986789302
max_steps : 2444
batch_size : 128
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.00015670315986789302
step:0
train performance: acc = 0.0625, loss = 2.3067749722782604
test performance: acc = 0.0962, loss = 2.3043040424272485
learning rate: 0.00015670315986789302
step:100
train performance: acc = 0.2109375, loss = 2.120921640093873
test performance: acc = 0.2502, loss = 2.114318242615059
learning rate: 0.00015670315986789302
step:200
train performance: acc = 0.3359375, loss = 1.9789640358885061
test performance: acc = 0.2988, loss = 1.9773233167971218
learning rate: 0.00015670315986789302
step:300
train performance: acc = 0.3203125, loss = 1.9096243565383546
test performance: acc = 0.3333, loss = 1.8930085341356215
learning rate: 0.00015670315986789302
step:400
train performance: acc = 0.34375, loss = 1.9109283072428778
test performance: acc = 0.3518, loss = 1.8343097285669598
learning rate: 0.00015670315986789302
step:500
train performance: acc = 0.3359375, loss = 1.7519958805335447
test performance: acc = 0.3721, loss = 1.7897522599158222
learning rate: 0.00015670315986789302
step:600
train performance: acc = 0.3671875, loss = 1.7342612833127375
test performance: acc = 0.3836, loss = 1.7504202864765583
learning rate: 0.00015670315986789302
step:700
train performance: acc = 0.4453125, loss = 1.6745458182560933
test performance: acc = 0.3941, loss = 1.7174246288714539
learning rate: 0.00015670315986789302
step:800
train performance: acc = 0.421875, loss = 1.6132853844001651
test performance: acc = 0.4052, loss = 1.6948314938970133
learning rate: 0.00015670315986789302
step:900
train performance: acc = 0.3671875, loss = 1.74919513016278
test performance: acc = 0.4107, loss = 1.669844363702309
learning rate: 0.00015670315986789302
step:1000
train performance: acc = 0.4609375, loss = 1.6730598908203698
test performance: acc = 0.4221, loss = 1.6480355493592604
learning rate: 0.00015670315986789302
step:1100
train performance: acc = 0.4375, loss = 1.5459462321369828
test performance: acc = 0.4322, loss = 1.6321389137833626
learning rate: 0.00015670315986789302
step:1200
train performance: acc = 0.4140625, loss = 1.586806281214098
test performance: acc = 0.4354, loss = 1.6139433671057322
learning rate: 0.00015670315986789302
step:1300
train performance: acc = 0.359375, loss = 1.7644435073385485
test performance: acc = 0.4355, loss = 1.6025259165801988
learning rate: 0.00015670315986789302
step:1400
train performance: acc = 0.5390625, loss = 1.4161592955880544
test performance: acc = 0.4443, loss = 1.5877185270042333
learning rate: 0.00015670315986789302
step:1500
train performance: acc = 0.40625, loss = 1.5549456982884025
test performance: acc = 0.4513, loss = 1.5765769976814195
learning rate: 0.00015670315986789302
step:1600
train performance: acc = 0.4921875, loss = 1.4596546799545376
test performance: acc = 0.4544, loss = 1.5674542298588905
learning rate: 0.00015670315986789302
step:1700
train performance: acc = 0.3984375, loss = 1.7590021564134521
test performance: acc = 0.4576, loss = 1.5567749679698801
learning rate: 0.00015670315986789302
step:1800
train performance: acc = 0.4921875, loss = 1.498374877722858
test performance: acc = 0.4632, loss = 1.5461221160898901
learning rate: 0.00015670315986789302
step:1900
train performance: acc = 0.4921875, loss = 1.579009148169358
test performance: acc = 0.4618, loss = 1.5343086129943622
learning rate: 0.00015670315986789302
step:2000
train performance: acc = 0.4765625, loss = 1.534051863847312
test performance: acc = 0.4653, loss = 1.527421338118804
learning rate: 0.00015670315986789302
step:2100
train performance: acc = 0.46875, loss = 1.5494556776465576
test performance: acc = 0.4697, loss = 1.5198674752219754
Early Stop
saving results in folder...
saving model in folder
ind 4 epoc 0
dnn_hidden_units : 50,10
learning_rate : 3.6124791622130425e-05
max_steps : 1257
batch_size : 929
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 3.6124791622130425e-05
step:0
train performance: acc = 0.11302475780409042, loss = 2.302589379102805
test performance: acc = 0.1001, loss = 2.3025880003225345
learning rate: 3.6124791622130425e-05
step:100
train performance: acc = 0.10118406889128095, loss = 2.302587528278205
test performance: acc = 0.1, loss = 2.3025879265946423
learning rate: 3.6124791622130425e-05
step:200
train performance: acc = 0.10441334768568353, loss = 2.302588269529699
test performance: acc = 0.1016, loss = 2.3025878528127732
learning rate: 3.6124791622130425e-05
step:300
train performance: acc = 0.11840688912809473, loss = 2.3025855345839896
test performance: acc = 0.1039, loss = 2.3025877786098055
learning rate: 3.6124791622130425e-05
step:400
train performance: acc = 0.09903121636167922, loss = 2.302587601058169
test performance: acc = 0.1023, loss = 2.3025877045779763
learning rate: 3.6124791622130425e-05
step:500
train performance: acc = 0.1108719052744887, loss = 2.3025865806351162
test performance: acc = 0.1027, loss = 2.3025876306943935
learning rate: 3.6124791622130425e-05
step:600
train performance: acc = 0.09472551130247578, loss = 2.3025885307121783
test performance: acc = 0.1028, loss = 2.3025875568730254
learning rate: 3.6124791622130425e-05
step:700
train performance: acc = 0.0968783638320775, loss = 2.3025865843655855
test performance: acc = 0.1025, loss = 2.302587482905147
learning rate: 3.6124791622130425e-05
step:800
train performance: acc = 0.11302475780409042, loss = 2.302586863884339
test performance: acc = 0.1041, loss = 2.302587408804431
learning rate: 3.6124791622130425e-05
step:900
train performance: acc = 0.11517761033369214, loss = 2.302587119315969
test performance: acc = 0.1044, loss = 2.302587334856943
learning rate: 3.6124791622130425e-05
step:1000
train performance: acc = 0.10871905274488698, loss = 2.302585656171691
test performance: acc = 0.1062, loss = 2.302587260714676
Early Stop
saving results in folder...
saving model in folder
ind 5 epoc 0
dnn_hidden_units : 300
learning_rate : 0.0018385087952471563
max_steps : 2207
batch_size : 115
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0018385087952471563
step:0
train performance: acc = 0.11304347826086956, loss = 2.3036758863724636
test performance: acc = 0.1554, loss = 2.288621438562347
learning rate: 0.0018385087952471563
step:100
train performance: acc = 0.3565217391304348, loss = 1.7850603869783663
test performance: acc = 0.3922, loss = 1.6945897594762187
learning rate: 0.0018385087952471563
step:200
train performance: acc = 0.41739130434782606, loss = 1.6058647534692327
test performance: acc = 0.4298, loss = 1.620400717009505
learning rate: 0.0018385087952471563
step:300
train performance: acc = 0.3826086956521739, loss = 1.719986001364443
test performance: acc = 0.4269, loss = 1.6428215787955542
learning rate: 0.0018385087952471563
step:400
train performance: acc = 0.4260869565217391, loss = 1.507902646133653
test performance: acc = 0.4085, loss = 1.6577069320752424
learning rate: 0.0018385087952471563
step:500
train performance: acc = 0.40869565217391307, loss = 1.6055965441995128
test performance: acc = 0.3948, loss = 1.748267428984117
learning rate: 0.0018385087952471563
step:600
train performance: acc = 0.4434782608695652, loss = 1.4784469938898543
test performance: acc = 0.4523, loss = 1.5595732100671318
learning rate: 0.0018385087952471563
step:700
train performance: acc = 0.40869565217391307, loss = 1.810035470988597
test performance: acc = 0.4465, loss = 1.5844001831923298
learning rate: 0.0018385087952471563
step:800
train performance: acc = 0.46956521739130436, loss = 1.4897354179960087
test performance: acc = 0.4399, loss = 1.620209240183128
learning rate: 0.0018385087952471563
step:900
train performance: acc = 0.4956521739130435, loss = 1.5042541180620983
test performance: acc = 0.4639, loss = 1.535778578027061
learning rate: 0.0018385087952471563
step:1000
train performance: acc = 0.4260869565217391, loss = 1.859070537512493
test performance: acc = 0.4261, loss = 1.6815442846880395
learning rate: 0.0018385087952471563
step:1100
train performance: acc = 0.4782608695652174, loss = 1.5349728233808062
test performance: acc = 0.4496, loss = 1.5766351324775292
Early Stop
saving results in folder...
saving model in folder
ind 6 epoc 0
dnn_hidden_units : 100
learning_rate : 0.0016926864110664238
max_steps : 1986
batch_size : 630
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0016926864110664238
step:0
train performance: acc = 0.07777777777777778, loss = 2.3042679028841033
test performance: acc = 0.176, loss = 2.2940853462647217
learning rate: 0.0016926864110664238
step:100
train performance: acc = 0.4380952380952381, loss = 1.6208961726181823
test performance: acc = 0.4031, loss = 1.6776003138948654
learning rate: 0.0016926864110664238
step:200
train performance: acc = 0.4507936507936508, loss = 1.573893155300462
test performance: acc = 0.4555, loss = 1.5578546362037573
learning rate: 0.0016926864110664238
step:300
train performance: acc = 0.48253968253968255, loss = 1.4880720250416593
test performance: acc = 0.4611, loss = 1.530303874384655
learning rate: 0.0016926864110664238
step:400
train performance: acc = 0.5, loss = 1.4215050276377588
test performance: acc = 0.4611, loss = 1.5537048751608526
learning rate: 0.0016926864110664238
step:500
train performance: acc = 0.5285714285714286, loss = 1.3221385489484476
test performance: acc = 0.4835, loss = 1.457804900575657
learning rate: 0.0016926864110664238
step:600
train performance: acc = 0.5222222222222223, loss = 1.3912577083001942
test performance: acc = 0.4794, loss = 1.4780445880375028
learning rate: 0.0016926864110664238
step:700
train performance: acc = 0.526984126984127, loss = 1.3486001899206925
test performance: acc = 0.4962, loss = 1.4349155646835496
learning rate: 0.0016926864110664238
step:800
train performance: acc = 0.5428571428571428, loss = 1.2953580429502451
test performance: acc = 0.4865, loss = 1.4610539598412056
learning rate: 0.0016926864110664238
step:900
train performance: acc = 0.5412698412698412, loss = 1.3267706952174307
test performance: acc = 0.4995, loss = 1.4219899962626374
learning rate: 0.0016926864110664238
step:1000
train performance: acc = 0.553968253968254, loss = 1.288650304736845
test performance: acc = 0.5041, loss = 1.392836870920866
learning rate: 0.0016926864110664238
step:1100
train performance: acc = 0.5476190476190477, loss = 1.368661022797374
test performance: acc = 0.4706, loss = 1.518393280207238
learning rate: 0.0016926864110664238
step:1200
train performance: acc = 0.5825396825396826, loss = 1.190541357693463
test performance: acc = 0.4977, loss = 1.452449828687413
learning rate: 0.0016926864110664238
step:1300
train performance: acc = 0.5746031746031746, loss = 1.1778471657864908
test performance: acc = 0.507, loss = 1.4164727888019892
learning rate: 0.0016926864110664238
step:1400
train performance: acc = 0.5682539682539682, loss = 1.276853814570595
test performance: acc = 0.4939, loss = 1.4371573903555963
learning rate: 0.0016926864110664238
step:1500
train performance: acc = 0.5365079365079365, loss = 1.260920648678612
test performance: acc = 0.4847, loss = 1.514439214324273
learning rate: 0.0016926864110664238
step:1600
train performance: acc = 0.6063492063492063, loss = 1.0882741450596385
test performance: acc = 0.5009, loss = 1.4528333810603022
learning rate: 0.0016926864110664238
step:1700
train performance: acc = 0.6015873015873016, loss = 1.1534155276333125
test performance: acc = 0.5082, loss = 1.4069912089575791
learning rate: 0.0016926864110664238
step:1800
train performance: acc = 0.5952380952380952, loss = 1.1661762341753996
test performance: acc = 0.508, loss = 1.4202534289148765
learning rate: 0.0016926864110664238
step:1900
train performance: acc = 0.6047619047619047, loss = 1.154112002119051
test performance: acc = 0.4966, loss = 1.4826726831132198
learning rate: 0.0016926864110664238
step:1985
train performance: acc = 0.6222222222222222, loss = 1.0981332335796703
test performance: acc = 0.5065, loss = 1.4531089034582936
saving results in folder...
saving model in folder
ind 7 epoc 0
dnn_hidden_units : 200
learning_rate : 0.0017639349232432147
max_steps : 2987
batch_size : 912
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0017639349232432147
step:0
train performance: acc = 0.08333333333333333, loss = 2.3027792380848298
test performance: acc = 0.1861, loss = 2.2861311792560883
learning rate: 0.0017639349232432147
step:100
train performance: acc = 0.4166666666666667, loss = 1.6421801227085866
test performance: acc = 0.4196, loss = 1.6432466474486487
learning rate: 0.0017639349232432147
step:200
train performance: acc = 0.48355263157894735, loss = 1.461835378099264
test performance: acc = 0.4666, loss = 1.5237946886212321
learning rate: 0.0017639349232432147
step:300
train performance: acc = 0.46600877192982454, loss = 1.5057687590953552
test performance: acc = 0.4799, loss = 1.473905663161549
learning rate: 0.0017639349232432147
step:400
train performance: acc = 0.4923245614035088, loss = 1.4190202709887936
test performance: acc = 0.4766, loss = 1.4948285678008502
learning rate: 0.0017639349232432147
step:500
train performance: acc = 0.518640350877193, loss = 1.3888330317522708
test performance: acc = 0.493, loss = 1.4420218605980322
learning rate: 0.0017639349232432147
step:600
train performance: acc = 0.543859649122807, loss = 1.2728557427689118
test performance: acc = 0.4823, loss = 1.4623904423239682
learning rate: 0.0017639349232432147
step:700
train performance: acc = 0.5625, loss = 1.294120876281188
test performance: acc = 0.5097, loss = 1.3941454994161533
learning rate: 0.0017639349232432147
step:800
train performance: acc = 0.5252192982456141, loss = 1.412615390923394
test performance: acc = 0.4798, loss = 1.5135285489215897
learning rate: 0.0017639349232432147
step:900
train performance: acc = 0.5603070175438597, loss = 1.2890686628546322
test performance: acc = 0.5099, loss = 1.4055277716623853
learning rate: 0.0017639349232432147
step:1000
train performance: acc = 0.5405701754385965, loss = 1.2578300727947238
test performance: acc = 0.4999, loss = 1.4289034903663065
learning rate: 0.0017639349232432147
step:1100
train performance: acc = 0.5274122807017544, loss = 1.3191966005597755
test performance: acc = 0.4903, loss = 1.4857641919942248
learning rate: 0.0017639349232432147
step:1200
train performance: acc = 0.5537280701754386, loss = 1.2920680916757639
test performance: acc = 0.5104, loss = 1.4091789428175918
learning rate: 0.0017639349232432147
step:1300
train performance: acc = 0.6118421052631579, loss = 1.080534737417235
test performance: acc = 0.4987, loss = 1.4812196170105898
learning rate: 0.0017639349232432147
step:1400
train performance: acc = 0.5701754385964912, loss = 1.2971145574735279
test performance: acc = 0.4805, loss = 1.5467878004214797
learning rate: 0.0017639349232432147
step:1500
train performance: acc = 0.6326754385964912, loss = 1.041018827929784
test performance: acc = 0.5241, loss = 1.3708074542546012
learning rate: 0.0017639349232432147
step:1600
train performance: acc = 0.5756578947368421, loss = 1.2237863856967799
test performance: acc = 0.49, loss = 1.5328738954021017
learning rate: 0.0017639349232432147
step:1700
train performance: acc = 0.5822368421052632, loss = 1.2295529208558826
test performance: acc = 0.4742, loss = 1.5852106360287213
learning rate: 0.0017639349232432147
step:1800
train performance: acc = 0.5745614035087719, loss = 1.2740800701100943
test performance: acc = 0.4948, loss = 1.5360530529664553
learning rate: 0.0017639349232432147
step:1900
train performance: acc = 0.6074561403508771, loss = 1.1440683772929865
test performance: acc = 0.4994, loss = 1.4634358377011376
learning rate: 0.0017639349232432147
step:2000
train performance: acc = 0.6469298245614035, loss = 1.0476233295690476
test performance: acc = 0.5157, loss = 1.4496344771699377
learning rate: 0.0017639349232432147
step:2100
train performance: acc = 0.581140350877193, loss = 1.1876679635619944
test performance: acc = 0.4866, loss = 1.5480235221925085
learning rate: 0.0017639349232432147
step:2200
train performance: acc = 0.5932017543859649, loss = 1.1909569814982177
test performance: acc = 0.5066, loss = 1.5015640771804748
learning rate: 0.0017639349232432147
step:2300
train performance: acc = 0.625, loss = 1.077270629923573
test performance: acc = 0.4952, loss = 1.538330160531034
learning rate: 0.0017639349232432147
step:2400
train performance: acc = 0.6085526315789473, loss = 1.1571796977287194
test performance: acc = 0.4729, loss = 1.762973328511658
learning rate: 0.0017639349232432147
step:2500
train performance: acc = 0.6633771929824561, loss = 0.9772036224798764
test performance: acc = 0.5281, loss = 1.4119628759216007
learning rate: 0.0017639349232432147
step:2600
train performance: acc = 0.6085526315789473, loss = 1.094749620319976
test performance: acc = 0.4787, loss = 1.6465670129870713
learning rate: 0.0017639349232432147
step:2700
train performance: acc = 0.6535087719298246, loss = 1.0730297276654535
test performance: acc = 0.4919, loss = 1.6132166314860525
learning rate: 0.0017639349232432147
step:2800
train performance: acc = 0.6644736842105263, loss = 0.9426933360703983
test performance: acc = 0.4991, loss = 1.5287072792552643
learning rate: 0.0017639349232432147
step:2900
train performance: acc = 0.6096491228070176, loss = 1.124850238761458
test performance: acc = 0.4822, loss = 1.6925336206427717
learning rate: 0.0017639349232432147
step:2986
train performance: acc = 0.6578947368421053, loss = 1.127704793140279
test performance: acc = 0.4699, loss = 1.7990616147518195
saving results in folder...
saving model in folder
ind 8 epoc 0
dnn_hidden_units : 100
learning_rate : 0.0017389209864205073
max_steps : 3468
batch_size : 997
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0017389209864205073
step:0
train performance: acc = 0.07923771313941826, loss = 2.3044956031309165
test performance: acc = 0.1731, loss = 2.2937554997710916
learning rate: 0.0017389209864205073
step:100
train performance: acc = 0.43029087261785354, loss = 1.6066447616289783
test performance: acc = 0.4183, loss = 1.6624609472494696
learning rate: 0.0017389209864205073
step:200
train performance: acc = 0.4563691073219659, loss = 1.5229922274506298
test performance: acc = 0.4576, loss = 1.5431185104585963
learning rate: 0.0017389209864205073
step:300
train performance: acc = 0.47843530591775324, loss = 1.4801316216969322
test performance: acc = 0.468, loss = 1.5112882125688225
learning rate: 0.0017389209864205073
step:400
train performance: acc = 0.4954864593781344, loss = 1.4198778884873169
test performance: acc = 0.4776, loss = 1.4882561855591776
learning rate: 0.0017389209864205073
step:500
train performance: acc = 0.5275827482447342, loss = 1.402554108342775
test performance: acc = 0.4867, loss = 1.4595831801199677
learning rate: 0.0017389209864205073
step:600
train performance: acc = 0.5155466399197592, loss = 1.387231900753996
test performance: acc = 0.4858, loss = 1.4683716516667675
learning rate: 0.0017389209864205073
step:700
train performance: acc = 0.5315947843530592, loss = 1.3190389012352908
test performance: acc = 0.4919, loss = 1.4551332784804112
learning rate: 0.0017389209864205073
step:800
train performance: acc = 0.5907723169508525, loss = 1.2715483967638372
test performance: acc = 0.5031, loss = 1.4064644698644087
learning rate: 0.0017389209864205073
step:900
train performance: acc = 0.5476429287863591, loss = 1.2869840609655303
test performance: acc = 0.4933, loss = 1.45064205651428
learning rate: 0.0017389209864205073
step:1000
train performance: acc = 0.551654964894684, loss = 1.2595668590531846
test performance: acc = 0.5061, loss = 1.4096681329238172
learning rate: 0.0017389209864205073
step:1100
train performance: acc = 0.5937813440320963, loss = 1.178910436276077
test performance: acc = 0.4952, loss = 1.4389676467122277
learning rate: 0.0017389209864205073
step:1200
train performance: acc = 0.5416248746238717, loss = 1.2930154811630983
test performance: acc = 0.4865, loss = 1.5000692005535146
learning rate: 0.0017389209864205073
step:1300
train performance: acc = 0.5857572718154463, loss = 1.2287866819673576
test performance: acc = 0.499, loss = 1.4330342741191648
learning rate: 0.0017389209864205073
step:1400
train performance: acc = 0.5837512537612839, loss = 1.2419096053202816
test performance: acc = 0.5033, loss = 1.4628429781082908
learning rate: 0.0017389209864205073
step:1500
train performance: acc = 0.6098294884653962, loss = 1.157004877739106
test performance: acc = 0.498, loss = 1.4212106355186869
learning rate: 0.0017389209864205073
step:1600
train performance: acc = 0.6058174523570712, loss = 1.1750362375620782
test performance: acc = 0.4999, loss = 1.456882442884311
learning rate: 0.0017389209864205073
step:1700
train performance: acc = 0.59679037111334, loss = 1.16877087613482
test performance: acc = 0.4843, loss = 1.5306556414192134
learning rate: 0.0017389209864205073
step:1800
train performance: acc = 0.6258776328986961, loss = 1.0946618208400865
test performance: acc = 0.4949, loss = 1.4750651410119502
learning rate: 0.0017389209864205073
step:1900
train performance: acc = 0.600802407221665, loss = 1.1210805392226657
test performance: acc = 0.507, loss = 1.4362113318548626
learning rate: 0.0017389209864205073
step:2000
train performance: acc = 0.6218655967903711, loss = 1.0870657915790458
test performance: acc = 0.5115, loss = 1.4284238285785138
learning rate: 0.0017389209864205073
step:2100
train performance: acc = 0.6168505516549649, loss = 1.1250209648277603
test performance: acc = 0.5011, loss = 1.464819146426311
learning rate: 0.0017389209864205073
step:2200
train performance: acc = 0.6188565697091274, loss = 1.132382589708128
test performance: acc = 0.4883, loss = 1.5338974241765568
learning rate: 0.0017389209864205073
step:2300
train performance: acc = 0.6098294884653962, loss = 1.123702262894608
test performance: acc = 0.4904, loss = 1.4797668525217944
learning rate: 0.0017389209864205073
step:2400
train performance: acc = 0.6108324974924775, loss = 1.116293968920909
test performance: acc = 0.4855, loss = 1.5391665805544712
learning rate: 0.0017389209864205073
step:2500
train performance: acc = 0.6208625877632898, loss = 1.1353029907986592
test performance: acc = 0.4871, loss = 1.542099654283788
learning rate: 0.0017389209864205073
step:2600
train performance: acc = 0.6058174523570712, loss = 1.2584677346147588
test performance: acc = 0.48, loss = 1.5781994886760173
Early Stop
saving results in folder...
saving model in folder
ind 9 epoc 0
dnn_hidden_units : 100
learning_rate : 0.0006192012143143677
max_steps : 3968
batch_size : 358
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0006192012143143677
step:0
train performance: acc = 0.0893854748603352, loss = 2.303569351493243
test performance: acc = 0.123, loss = 2.3000843320044138
learning rate: 0.0006192012143143677
step:100
train performance: acc = 0.4106145251396648, loss = 1.8895886914952411
test performance: acc = 0.3259, loss = 1.876036204807003
learning rate: 0.0006192012143143677
step:200
train performance: acc = 0.329608938547486, loss = 1.749566234204592
test performance: acc = 0.3848, loss = 1.7298174625123626
learning rate: 0.0006192012143143677
step:300
train performance: acc = 0.4301675977653631, loss = 1.6136385108208162
test performance: acc = 0.4192, loss = 1.6519076667541135
learning rate: 0.0006192012143143677
step:400
train performance: acc = 0.43575418994413406, loss = 1.5832842960159947
test performance: acc = 0.4379, loss = 1.5996284715728415
learning rate: 0.0006192012143143677
step:500
train performance: acc = 0.43854748603351956, loss = 1.5730252663270043
test performance: acc = 0.4518, loss = 1.5599107613344614
learning rate: 0.0006192012143143677
step:600
train performance: acc = 0.4720670391061452, loss = 1.4980337544609315
test performance: acc = 0.4665, loss = 1.534205360949472
learning rate: 0.0006192012143143677
step:700
train performance: acc = 0.4692737430167598, loss = 1.4246225418249083
test performance: acc = 0.4673, loss = 1.5140100441464859
learning rate: 0.0006192012143143677
step:800
train performance: acc = 0.505586592178771, loss = 1.4665630609565343
test performance: acc = 0.482, loss = 1.490416143446176
learning rate: 0.0006192012143143677
step:900
train performance: acc = 0.5391061452513967, loss = 1.3413078349451326
test performance: acc = 0.486, loss = 1.4661203635168696
learning rate: 0.0006192012143143677
step:1000
train performance: acc = 0.5307262569832403, loss = 1.3545807362676863
test performance: acc = 0.4905, loss = 1.456253242018066
learning rate: 0.0006192012143143677
step:1100
train performance: acc = 0.4972067039106145, loss = 1.4116568405447107
test performance: acc = 0.4889, loss = 1.4552104677287017
learning rate: 0.0006192012143143677
step:1200
train performance: acc = 0.5, loss = 1.4200488180176587
test performance: acc = 0.4943, loss = 1.4390050162353907
learning rate: 0.0006192012143143677
step:1300
train performance: acc = 0.5279329608938548, loss = 1.3398838777070308
test performance: acc = 0.4973, loss = 1.4310406256896948
learning rate: 0.0006192012143143677
step:1400
train performance: acc = 0.5418994413407822, loss = 1.3198176695924035
test performance: acc = 0.5001, loss = 1.4301982358439926
learning rate: 0.0006192012143143677
step:1500
train performance: acc = 0.5558659217877095, loss = 1.288376158226497
test performance: acc = 0.5058, loss = 1.4143252395104233
learning rate: 0.0006192012143143677
step:1600
train performance: acc = 0.5670391061452514, loss = 1.2250427138511861
test performance: acc = 0.5077, loss = 1.4099457093742538
learning rate: 0.0006192012143143677
step:1700
train performance: acc = 0.5949720670391061, loss = 1.2035197069819334
test performance: acc = 0.5092, loss = 1.4038256383628072
learning rate: 0.0006192012143143677
step:1800
train performance: acc = 0.547486033519553, loss = 1.305349306909502
test performance: acc = 0.5082, loss = 1.399319677700047
learning rate: 0.0006192012143143677
step:1900
train performance: acc = 0.5698324022346368, loss = 1.2529450052165463
test performance: acc = 0.5032, loss = 1.3951160150631854
learning rate: 0.0006192012143143677
step:2000
train performance: acc = 0.5865921787709497, loss = 1.2336814066916815
test performance: acc = 0.5034, loss = 1.4169316545630959
learning rate: 0.0006192012143143677
step:2100
train performance: acc = 0.5810055865921788, loss = 1.1541371990406508
test performance: acc = 0.5098, loss = 1.4004513991773295
learning rate: 0.0006192012143143677
step:2200
train performance: acc = 0.5586592178770949, loss = 1.2449591851749968
test performance: acc = 0.5107, loss = 1.383451276794174
learning rate: 0.0006192012143143677
step:2300
train performance: acc = 0.5698324022346368, loss = 1.2205190632750194
test performance: acc = 0.5114, loss = 1.3973723802635534
learning rate: 0.0006192012143143677
step:2400
train performance: acc = 0.5614525139664804, loss = 1.2045384567622146
test performance: acc = 0.5047, loss = 1.411682419486081
learning rate: 0.0006192012143143677
step:2500
train performance: acc = 0.5586592178770949, loss = 1.1926392906018772
test performance: acc = 0.5188, loss = 1.384146923011727
learning rate: 0.0006192012143143677
step:2600
train performance: acc = 0.6005586592178771, loss = 1.1338616010824645
test performance: acc = 0.5126, loss = 1.392199349554323
learning rate: 0.0006192012143143677
step:2700
train performance: acc = 0.5893854748603352, loss = 1.226036673207107
test performance: acc = 0.5083, loss = 1.397097031882163
learning rate: 0.0006192012143143677
step:2800
train performance: acc = 0.6312849162011173, loss = 1.0570632024665743
test performance: acc = 0.51, loss = 1.3983245632280912
learning rate: 0.0006192012143143677
step:2900
train performance: acc = 0.6061452513966481, loss = 1.1790882841073786
test performance: acc = 0.5159, loss = 1.3808156163113747
learning rate: 0.0006192012143143677
step:3000
train performance: acc = 0.5949720670391061, loss = 1.22980433890276
test performance: acc = 0.5151, loss = 1.3851202838459757
learning rate: 0.0006192012143143677
step:3100
train performance: acc = 0.6061452513966481, loss = 1.165867424515856
test performance: acc = 0.5114, loss = 1.420432574865323
learning rate: 0.0006192012143143677
step:3200
train performance: acc = 0.61731843575419, loss = 1.0398192632659538
test performance: acc = 0.5175, loss = 1.3974232801310145
learning rate: 0.0006192012143143677
step:3300
train performance: acc = 0.611731843575419, loss = 1.0765014210305996
test performance: acc = 0.5107, loss = 1.4112704857169793
learning rate: 0.0006192012143143677
step:3400
train performance: acc = 0.6256983240223464, loss = 1.130678935545273
test performance: acc = 0.5024, loss = 1.4422909360175982
learning rate: 0.0006192012143143677
step:3500
train performance: acc = 0.6145251396648045, loss = 1.1085884500953673
test performance: acc = 0.5074, loss = 1.427803265758141
learning rate: 0.0006192012143143677
step:3600
train performance: acc = 0.611731843575419, loss = 1.054757799788845
test performance: acc = 0.5142, loss = 1.393701629107427
learning rate: 0.0006192012143143677
step:3700
train performance: acc = 0.6201117318435754, loss = 1.120217143612116
test performance: acc = 0.5217, loss = 1.3949091375496852
learning rate: 0.0006192012143143677
step:3800
train performance: acc = 0.6759776536312849, loss = 0.9816861026933722
test performance: acc = 0.5133, loss = 1.4057515680514414
learning rate: 0.0006192012143143677
step:3900
train performance: acc = 0.6089385474860335, loss = 1.0968620982505999
test performance: acc = 0.5059, loss = 1.434839689002172
learning rate: 0.0006192012143143677
step:3967
train performance: acc = 0.61731843575419, loss = 1.083883516473851
test performance: acc = 0.4973, loss = 1.465540780892665
saving results in folder...
saving model in folder
accuracies: [0.45569999999999999, 0.47420000000000001, 0.19889999999999999, 0.46970000000000001, 0.1062, 0.4496, 0.50649999999999995, 0.46989999999999998, 0.47999999999999998, 0.49730000000000002]
ind 0 epoc 1
dnn_hidden_units : 200
learning_rate : 0.00647245141195
max_steps : 3739
batch_size : 464
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0064724514119455685
step:0
train performance: acc = 0.07543103448275862, loss = 2.303406092976601
test performance: acc = 0.1955, loss = 2.2446155986294585
learning rate: 0.0064724514119455685
step:100
train performance: acc = 0.23060344827586207, loss = 6.888031107722691
test performance: acc = 0.2301, loss = 7.443418191160474
learning rate: 0.0064724514119455685
step:200
train performance: acc = 0.23922413793103448, loss = 6.125552769490467
test performance: acc = 0.1832, loss = 9.025491344957299
learning rate: 0.0064724514119455685
step:300
train performance: acc = 0.22629310344827586, loss = 7.3417037409899715
test performance: acc = 0.1864, loss = 7.696217518744322
learning rate: 0.0064724514119455685
step:400
train performance: acc = 0.25646551724137934, loss = 6.794885044553806
test performance: acc = 0.2237, loss = 8.008013258862784
learning rate: 0.0064724514119455685
step:500
train performance: acc = 0.23275862068965517, loss = 7.314390982577259
test performance: acc = 0.2356, loss = 8.352293824225859
learning rate: 0.0064724514119455685
step:600
train performance: acc = 0.2543103448275862, loss = 7.3994672339157335
test performance: acc = 0.2186, loss = 8.243863587987772
learning rate: 0.0064724514119455685
step:700
train performance: acc = 0.2952586206896552, loss = 6.495086142879157
test performance: acc = 0.2252, loss = 8.841771802346143
learning rate: 0.0064724514119455685
step:800
train performance: acc = 0.3017241379310345, loss = 5.822434953154394
test performance: acc = 0.2924, loss = 6.962781572466186
learning rate: 0.0064724514119455685
step:900
train performance: acc = 0.3017241379310345, loss = 5.463673551313397
test performance: acc = 0.2823, loss = 7.315495423271667
learning rate: 0.0064724514119455685
step:1000
train performance: acc = 0.34698275862068967, loss = 5.969361920056778
test performance: acc = 0.2822, loss = 6.346391084688108
learning rate: 0.0064724514119455685
step:1100
train performance: acc = 0.28448275862068967, loss = 6.150317362427419
test performance: acc = 0.2482, loss = 7.478452088334004
learning rate: 0.0064724514119455685
step:1200
train performance: acc = 0.29094827586206895, loss = 6.762328022678719
test performance: acc = 0.2784, loss = 6.601633681767655
learning rate: 0.0064724514119455685
step:1300
train performance: acc = 0.3426724137931034, loss = 4.480204631822018
test performance: acc = 0.2476, loss = 8.02040406687189
learning rate: 0.0064724514119455685
step:1400
train performance: acc = 0.27370689655172414, loss = 7.713974357058017
test performance: acc = 0.3213, loss = 6.64011255362619
learning rate: 0.0064724514119455685
step:1500
train performance: acc = 0.34698275862068967, loss = 5.477399246600772
test performance: acc = 0.3373, loss = 5.704040563188063
learning rate: 0.0064724514119455685
step:1600
train performance: acc = 0.30603448275862066, loss = 7.139430037232677
test performance: acc = 0.2785, loss = 6.886353385489545
Early Stop
saving results in folder...
saving model in folder
ind 1 epoc 1
dnn_hidden_units : 50,10
learning_rate : 1e-05
max_steps : 4063
batch_size : 1223
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 1e-05
step:0
train performance: acc = 0.10956663941128372, loss = 2.3025891483873107
test performance: acc = 0.1001, loss = 2.3025880009351267
learning rate: 1e-05
step:100
train performance: acc = 0.11529026982829109, loss = 2.3025863200062977
test performance: acc = 0.1002, loss = 2.302587980505842
learning rate: 1e-05
step:200
train performance: acc = 0.09975470155355683, loss = 2.3025876265036094
test performance: acc = 0.1004, loss = 2.302587960096831
learning rate: 1e-05
step:300
train performance: acc = 0.11120196238757155, loss = 2.30258653337808
test performance: acc = 0.1004, loss = 2.3025879396170206
learning rate: 1e-05
step:400
train performance: acc = 0.10384300899427637, loss = 2.3025867385744285
test performance: acc = 0.1003, loss = 2.302587919218613
learning rate: 1e-05
step:500
train performance: acc = 0.10629599345870809, loss = 2.3025872740380273
test performance: acc = 0.1006, loss = 2.302587898800644
learning rate: 1e-05
step:600
train performance: acc = 0.09239574816026165, loss = 2.3025874769285912
test performance: acc = 0.1006, loss = 2.302587878292587
learning rate: 1e-05
step:700
train performance: acc = 0.11038430089942763, loss = 2.302587115669059
test performance: acc = 0.1008, loss = 2.3025878578229237
learning rate: 1e-05
step:800
train performance: acc = 0.10220768601798855, loss = 2.3025863724942104
test performance: acc = 0.1011, loss = 2.3025878373829975
learning rate: 1e-05
step:900
train performance: acc = 0.10793131643499591, loss = 2.302587470621771
test performance: acc = 0.1013, loss = 2.302587816918204
learning rate: 1e-05
step:1000
train performance: acc = 0.10302534750613246, loss = 2.3025872513722603
test performance: acc = 0.1013, loss = 2.302587796471561
Early Stop
saving results in folder...
saving model in folder
ind 2 epoc 1
dnn_hidden_units : 200
learning_rate : 0.0144262240268
max_steps : 3666
batch_size : 847
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.014426224026794002
step:0
train performance: acc = 0.08382526564344746, loss = 2.3026928471944808
test performance: acc = 0.2203, loss = 2.193668339661183
learning rate: 0.014426224026794002
step:100
train performance: acc = 0.2077922077922078, loss = 8.728655219677881
test performance: acc = 0.2417, loss = 8.267368962601676
learning rate: 0.014426224026794002
step:200
train performance: acc = 0.3010625737898465, loss = 8.213766458461963
test performance: acc = 0.2446, loss = 8.890309362433543
learning rate: 0.014426224026794002
step:300
train performance: acc = 0.26092089728453366, loss = 8.70421930229406
test performance: acc = 0.2704, loss = 8.36614615502383
learning rate: 0.014426224026794002
step:400
train performance: acc = 0.24439197166469895, loss = 9.326557014655737
test performance: acc = 0.2454, loss = 9.313589313156749
learning rate: 0.014426224026794002
step:500
train performance: acc = 0.2680047225501771, loss = 9.245437567535799
test performance: acc = 0.2737, loss = 8.587704088120645
learning rate: 0.014426224026794002
step:600
train performance: acc = 0.33766233766233766, loss = 7.673987257691904
test performance: acc = 0.2895, loss = 8.631885189120768
learning rate: 0.014426224026794002
step:700
train performance: acc = 0.31995277449822906, loss = 8.419286297124858
test performance: acc = 0.3098, loss = 8.206386844522246
learning rate: 0.014426224026794002
step:800
train performance: acc = 0.3789846517119244, loss = 7.637860871627677
test performance: acc = 0.3351, loss = 7.966301340065099
learning rate: 0.014426224026794002
step:900
train performance: acc = 0.3707201889020071, loss = 7.828269173798158
test performance: acc = 0.3419, loss = 7.867565790973005
learning rate: 0.014426224026794002
step:1000
train performance: acc = 0.4014167650531287, loss = 7.062360584159045
test performance: acc = 0.3223, loss = 8.50966039764033
learning rate: 0.014426224026794002
step:1100
train performance: acc = 0.32585596221959856, loss = 8.676575720530153
test performance: acc = 0.3278, loss = 8.436268774559512
learning rate: 0.014426224026794002
step:1200
train performance: acc = 0.3659976387249115, loss = 8.106437306971284
test performance: acc = 0.3676, loss = 7.846800437216848
learning rate: 0.014426224026794002
step:1300
train performance: acc = 0.3872491145218418, loss = 7.816465445288553
test performance: acc = 0.3241, loss = 8.401989107042647
learning rate: 0.014426224026794002
step:1400
train performance: acc = 0.345926800472255, loss = 8.443828600543284
test performance: acc = 0.3588, loss = 7.775056435660439
learning rate: 0.014426224026794002
step:1500
train performance: acc = 0.38488783943329397, loss = 7.721428370427869
test performance: acc = 0.3253, loss = 8.514068871766451
Early Stop
saving results in folder...
saving model in folder
ind 3 epoc 1
dnn_hidden_units : 200
learning_rate : 0.000738857437231
max_steps : 2641
batch_size : 791
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0007388574372312488
step:0
train performance: acc = 0.08343868520859671, loss = 2.3026860219712137
test performance: acc = 0.1472, loss = 2.2971812650081564
learning rate: 0.0007388574372312488
step:100
train performance: acc = 0.3691529709228824, loss = 1.8063992933379367
test performance: acc = 0.3648, loss = 1.7991275002668465
learning rate: 0.0007388574372312488
step:200
train performance: acc = 0.4108723135271808, loss = 1.63326664570443
test performance: acc = 0.4181, loss = 1.6588358135335532
learning rate: 0.0007388574372312488
step:300
train performance: acc = 0.4450063211125158, loss = 1.5908579290489722
test performance: acc = 0.4498, loss = 1.5835753748747412
learning rate: 0.0007388574372312488
step:400
train performance: acc = 0.49557522123893805, loss = 1.5227844942127067
test performance: acc = 0.4641, loss = 1.5367419161166795
learning rate: 0.0007388574372312488
step:500
train performance: acc = 0.504424778761062, loss = 1.4667573451823586
test performance: acc = 0.4775, loss = 1.4979540423440258
learning rate: 0.0007388574372312488
step:600
train performance: acc = 0.5284450063211125, loss = 1.4224302224891654
test performance: acc = 0.4844, loss = 1.4702856620706788
learning rate: 0.0007388574372312488
step:700
train performance: acc = 0.5309734513274337, loss = 1.3541347601689906
test performance: acc = 0.4949, loss = 1.4504795910151465
learning rate: 0.0007388574372312488
step:800
train performance: acc = 0.5259165613147914, loss = 1.3740658727109532
test performance: acc = 0.5, loss = 1.4320722910269092
learning rate: 0.0007388574372312488
step:900
train performance: acc = 0.5322376738305942, loss = 1.3422299918858005
test performance: acc = 0.5033, loss = 1.4155295491028692
learning rate: 0.0007388574372312488
step:1000
train performance: acc = 0.5486725663716814, loss = 1.376001427148811
test performance: acc = 0.5068, loss = 1.3993796318713156
learning rate: 0.0007388574372312488
step:1100
train performance: acc = 0.5524652338811631, loss = 1.2621078048191618
test performance: acc = 0.5169, loss = 1.3869082704475642
learning rate: 0.0007388574372312488
step:1200
train performance: acc = 0.5701643489254109, loss = 1.2309576260755848
test performance: acc = 0.5138, loss = 1.3843755208013242
learning rate: 0.0007388574372312488
step:1300
train performance: acc = 0.5562579013906448, loss = 1.2587210203895924
test performance: acc = 0.518, loss = 1.381177537475803
learning rate: 0.0007388574372312488
step:1400
train performance: acc = 0.5651074589127687, loss = 1.2395632894996471
test performance: acc = 0.5249, loss = 1.3605631475260929
learning rate: 0.0007388574372312488
step:1500
train performance: acc = 0.5815423514538559, loss = 1.2168509478043577
test performance: acc = 0.5229, loss = 1.3674108097446587
learning rate: 0.0007388574372312488
step:1600
train performance: acc = 0.5967130214917825, loss = 1.1440432247913868
test performance: acc = 0.5198, loss = 1.373289534347948
learning rate: 0.0007388574372312488
step:1700
train performance: acc = 0.6005056890012642, loss = 1.1677656925811186
test performance: acc = 0.527, loss = 1.3539548418666392
learning rate: 0.0007388574372312488
step:1800
train performance: acc = 0.5891276864728192, loss = 1.181398922345851
test performance: acc = 0.5278, loss = 1.358305482972398
learning rate: 0.0007388574372312488
step:1900
train performance: acc = 0.6042983565107459, loss = 1.1637038979471839
test performance: acc = 0.5166, loss = 1.3930134184472691
learning rate: 0.0007388574372312488
step:2000
train performance: acc = 0.5676359039190898, loss = 1.2077908672888387
test performance: acc = 0.5103, loss = 1.4197363124903801
learning rate: 0.0007388574372312488
step:2100
train performance: acc = 0.6093552465233881, loss = 1.1371615064936822
test performance: acc = 0.5282, loss = 1.352816603837639
learning rate: 0.0007388574372312488
step:2200
train performance: acc = 0.6118836915297092, loss = 1.1189431194896373
test performance: acc = 0.522, loss = 1.369715972579634
learning rate: 0.0007388574372312488
step:2300
train performance: acc = 0.6333754740834386, loss = 1.087152636708278
test performance: acc = 0.5207, loss = 1.3861386002249116
learning rate: 0.0007388574372312488
step:2400
train performance: acc = 0.6536030341340076, loss = 1.03182896643989
test performance: acc = 0.5174, loss = 1.3738810365565286
learning rate: 0.0007388574372312488
step:2500
train performance: acc = 0.6283185840707964, loss = 1.0521646665888043
test performance: acc = 0.5252, loss = 1.3613180130398406
learning rate: 0.0007388574372312488
step:2600
train performance: acc = 0.629582806573957, loss = 1.065654243647713
test performance: acc = 0.5237, loss = 1.3848427924812186
learning rate: 0.0007388574372312488
step:2640
train performance: acc = 0.6447534766118836, loss = 1.1382595107076545
test performance: acc = 0.4992, loss = 1.4810412866261518
saving results in folder...
saving model in folder
ind 4 epoc 1
dnn_hidden_units : 100,10
learning_rate : 0.000125417755104
max_steps : 2179
batch_size : 988
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.000125417755103584
step:0
train performance: acc = 0.10931174089068826, loss = 2.302585711548612
test performance: acc = 0.1091, loss = 2.302583052936023
learning rate: 0.000125417755103584
step:100
train performance: acc = 0.1214574898785425, loss = 2.302582406342269
test performance: acc = 0.1093, loss = 2.302582333386167
learning rate: 0.000125417755103584
step:200
train performance: acc = 0.10425101214574899, loss = 2.3025799855575597
test performance: acc = 0.1085, loss = 2.302581607681535
learning rate: 0.000125417755103584
step:300
train performance: acc = 0.10728744939271255, loss = 2.3025823926006543
test performance: acc = 0.1085, loss = 2.3025808769983196
learning rate: 0.000125417755103584
step:400
train performance: acc = 0.11538461538461539, loss = 2.302580173679494
test performance: acc = 0.1093, loss = 2.302580138928856
learning rate: 0.000125417755103584
step:500
train performance: acc = 0.10526315789473684, loss = 2.302577485419821
test performance: acc = 0.1102, loss = 2.30257939267256
learning rate: 0.000125417755103584
step:600
train performance: acc = 0.10728744939271255, loss = 2.302578520705458
test performance: acc = 0.1112, loss = 2.3025786389665632
learning rate: 0.000125417755103584
step:700
train performance: acc = 0.11336032388663968, loss = 2.302577877822465
test performance: acc = 0.1098, loss = 2.302577874982619
learning rate: 0.000125417755103584
step:800
train performance: acc = 0.11032388663967611, loss = 2.302575904248529
test performance: acc = 0.1099, loss = 2.302577102119923
learning rate: 0.000125417755103584
step:900
train performance: acc = 0.12449392712550607, loss = 2.302574423133865
test performance: acc = 0.11, loss = 2.302576321264006
learning rate: 0.000125417755103584
step:1000
train performance: acc = 0.11538461538461539, loss = 2.302574678803387
test performance: acc = 0.1113, loss = 2.302575529055403
learning rate: 0.000125417755103584
step:1100
train performance: acc = 0.10121457489878542, loss = 2.302572901322684
test performance: acc = 0.1118, loss = 2.302574723453079
learning rate: 0.000125417755103584
step:1200
train performance: acc = 0.09817813765182186, loss = 2.3025759008536184
test performance: acc = 0.1131, loss = 2.3025739059036296
learning rate: 0.000125417755103584
step:1300
train performance: acc = 0.1214574898785425, loss = 2.3025700789551795
test performance: acc = 0.1125, loss = 2.30257307520546
learning rate: 0.000125417755103584
step:1400
train performance: acc = 0.11234817813765183, loss = 2.302572930156055
test performance: acc = 0.1121, loss = 2.3025722303757465
learning rate: 0.000125417755103584
step:1500
train performance: acc = 0.10526315789473684, loss = 2.3025748841078735
test performance: acc = 0.1134, loss = 2.302571370178649
learning rate: 0.000125417755103584
step:1600
train performance: acc = 0.11538461538461539, loss = 2.3025715669204336
test performance: acc = 0.1144, loss = 2.3025704949774846
learning rate: 0.000125417755103584
step:1700
train performance: acc = 0.11740890688259109, loss = 2.3025671061201565
test performance: acc = 0.1125, loss = 2.302569600416167
learning rate: 0.000125417755103584
step:1800
train performance: acc = 0.12550607287449392, loss = 2.3025670996813785
test performance: acc = 0.1142, loss = 2.3025686887409287
learning rate: 0.000125417755103584
step:1900
train performance: acc = 0.12246963562753037, loss = 2.3025680288383215
test performance: acc = 0.1142, loss = 2.3025677553272264
learning rate: 0.000125417755103584
step:2000
train performance: acc = 0.09412955465587045, loss = 2.302574043562403
test performance: acc = 0.1172, loss = 2.3025668017761873
learning rate: 0.000125417755103584
step:2100
train performance: acc = 0.1346153846153846, loss = 2.302561044218329
test performance: acc = 0.1144, loss = 2.302565819196808
learning rate: 0.000125417755103584
step:2178
train performance: acc = 0.12348178137651822, loss = 2.3025650687950745
test performance: acc = 0.119, loss = 2.3025650391212222
saving results in folder...
saving model in folder
ind 5 epoc 1
dnn_hidden_units : 100
learning_rate : 0.0144717566291
max_steps : 3296
batch_size : 1687
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.014471756629100269
step:0
train performance: acc = 0.07705986959098993, loss = 2.3037616934486382
test performance: acc = 0.2228, loss = 2.2347955637138854
learning rate: 0.014471756629100269
step:100
train performance: acc = 0.26081802015411976, loss = 7.753209846792058
test performance: acc = 0.2691, loss = 7.243354620658636
learning rate: 0.014471756629100269
step:200
train performance: acc = 0.23829282750444578, loss = 8.693835348183967
test performance: acc = 0.2609, loss = 8.072701180564117
learning rate: 0.014471756629100269
step:300
train performance: acc = 0.2886781268524007, loss = 7.878236329522443
test performance: acc = 0.2563, loss = 8.131736682638826
learning rate: 0.014471756629100269
step:400
train performance: acc = 0.3005334914048607, loss = 7.03285783774975
test performance: acc = 0.3176, loss = 6.884472053833526
learning rate: 0.014471756629100269
step:500
train performance: acc = 0.2981624184943687, loss = 7.163679318466126
test performance: acc = 0.3091, loss = 7.226344261509266
learning rate: 0.014471756629100269
step:600
train performance: acc = 0.3372851215174867, loss = 6.4427708282950595
test performance: acc = 0.293, loss = 7.488633482294998
learning rate: 0.014471756629100269
step:700
train performance: acc = 0.34143449911084767, loss = 6.80548204508915
test performance: acc = 0.2907, loss = 7.7863548272433425
learning rate: 0.014471756629100269
step:800
train performance: acc = 0.33017190278601066, loss = 7.183302510874357
test performance: acc = 0.2932, loss = 8.044224940111778
learning rate: 0.014471756629100269
step:900
train performance: acc = 0.36218138707765263, loss = 6.581821706683912
test performance: acc = 0.3497, loss = 6.886930207818885
learning rate: 0.014471756629100269
step:1000
train performance: acc = 0.3912270302311796, loss = 6.347429690083047
test performance: acc = 0.3639, loss = 6.688624813933995
learning rate: 0.014471756629100269
step:1100
train performance: acc = 0.36870183758150565, loss = 6.569925679245635
test performance: acc = 0.3342, loss = 7.246570209939757
learning rate: 0.014471756629100269
step:1200
train performance: acc = 0.38529934795494963, loss = 6.5979268680623875
test performance: acc = 0.3593, loss = 6.872033649144178
learning rate: 0.014471756629100269
step:1300
train performance: acc = 0.3823355068168346, loss = 6.616292119498088
test performance: acc = 0.3515, loss = 7.242569058609039
learning rate: 0.014471756629100269
step:1400
train performance: acc = 0.4303497332542976, loss = 5.931906316335929
test performance: acc = 0.3675, loss = 6.795857093658888
learning rate: 0.014471756629100269
step:1500
train performance: acc = 0.38292827504445764, loss = 6.594455103085384
test performance: acc = 0.3536, loss = 7.1861495558587265
learning rate: 0.014471756629100269
step:1600
train performance: acc = 0.37166567871962064, loss = 6.920000306267454
test performance: acc = 0.3479, loss = 7.395634116200391
learning rate: 0.014471756629100269
step:1700
train performance: acc = 0.4475400118553646, loss = 5.4211103883668255
test performance: acc = 0.3605, loss = 7.250959185266163
learning rate: 0.014471756629100269
step:1800
train performance: acc = 0.4599881446354475, loss = 5.689446821439721
test performance: acc = 0.3362, loss = 7.668463688194928
learning rate: 0.014471756629100269
step:1900
train performance: acc = 0.41197391819798457, loss = 6.37065615964591
test performance: acc = 0.3761, loss = 7.116388533243437
learning rate: 0.014471756629100269
step:2000
train performance: acc = 0.42442205097806757, loss = 6.381588789567698
test performance: acc = 0.3455, loss = 7.470404191243421
learning rate: 0.014471756629100269
step:2100
train performance: acc = 0.44042679312388855, loss = 6.235506816454392
test performance: acc = 0.3745, loss = 7.084555919194996
learning rate: 0.014471756629100269
step:2200
train performance: acc = 0.41256668642560756, loss = 6.1778523274624515
test performance: acc = 0.3804, loss = 7.036341496890489
learning rate: 0.014471756629100269
step:2300
train performance: acc = 0.45050385299347956, loss = 6.2608740043964515
test performance: acc = 0.379, loss = 7.044652306384347
Early Stop
saving results in folder...
saving model in folder
ind 7 epoc 1
dnn_hidden_units : 200
learning_rate : 0.00209015582123
max_steps : 3340
batch_size : 944
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.00209015582122738
step:0
train performance: acc = 0.0815677966101695, loss = 2.3030228012474665
test performance: acc = 0.1955, loss = 2.2828422012134286
learning rate: 0.00209015582122738
step:100
train performance: acc = 0.4269067796610169, loss = 1.6295120406387944
test performance: acc = 0.4342, loss = 1.6098087020427663
learning rate: 0.00209015582122738
step:200
train performance: acc = 0.4226694915254237, loss = 1.5554517537514139
test performance: acc = 0.456, loss = 1.549991044045494
learning rate: 0.00209015582122738
step:300
train performance: acc = 0.4851694915254237, loss = 1.456955906693967
test performance: acc = 0.4672, loss = 1.5217785753784938
learning rate: 0.00209015582122738
step:400
train performance: acc = 0.4915254237288136, loss = 1.4605267273747684
test performance: acc = 0.4751, loss = 1.4809489443108796
learning rate: 0.00209015582122738
step:500
train performance: acc = 0.4586864406779661, loss = 1.6139024186856439
test performance: acc = 0.471, loss = 1.5439575663019622
learning rate: 0.00209015582122738
step:600
train performance: acc = 0.5180084745762712, loss = 1.3691182267970676
test performance: acc = 0.4896, loss = 1.4563906899590118
learning rate: 0.00209015582122738
step:700
train performance: acc = 0.5211864406779662, loss = 1.3111463999445387
test performance: acc = 0.5112, loss = 1.3937050196713412
learning rate: 0.00209015582122738
step:800
train performance: acc = 0.5529661016949152, loss = 1.2837460324256296
test performance: acc = 0.5061, loss = 1.4376903928669669
learning rate: 0.00209015582122738
step:900
train performance: acc = 0.5614406779661016, loss = 1.30425803739918
test performance: acc = 0.4915, loss = 1.4569122393212812
learning rate: 0.00209015582122738
step:1000
train performance: acc = 0.559322033898305, loss = 1.223198175197513
test performance: acc = 0.4843, loss = 1.5021211941587997
learning rate: 0.00209015582122738
step:1100
train performance: acc = 0.5614406779661016, loss = 1.234139107496162
test performance: acc = 0.4961, loss = 1.4733790330852343
learning rate: 0.00209015582122738
step:1200
train performance: acc = 0.5667372881355932, loss = 1.293124798004757
test performance: acc = 0.4832, loss = 1.482900550522916
learning rate: 0.00209015582122738
step:1300
train performance: acc = 0.5836864406779662, loss = 1.169484233558455
test performance: acc = 0.4959, loss = 1.4487782779995126
learning rate: 0.00209015582122738
step:1400
train performance: acc = 0.5529661016949152, loss = 1.2874830847851253
test performance: acc = 0.4853, loss = 1.5741832574591135
learning rate: 0.00209015582122738
step:1500
train performance: acc = 0.5169491525423728, loss = 1.6239912248584145
test performance: acc = 0.4315, loss = 2.047174224088985
Early Stop
saving results in folder...
saving model in folder
ind 8 epoc 1
dnn_hidden_units : 100
learning_rate : 0.00172295357288
max_steps : 2728
batch_size : 1311
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0017229535728773408
step:0
train performance: acc = 0.08237986270022883, loss = 2.3038916360052197
test performance: acc = 0.1703, loss = 2.2939068538072074
learning rate: 0.0017229535728773408
step:100
train performance: acc = 0.38138825324180015, loss = 1.6622392181581938
test performance: acc = 0.4182, loss = 1.6609594445819698
learning rate: 0.0017229535728773408
step:200
train performance: acc = 0.4569031273836766, loss = 1.5663732536926473
test performance: acc = 0.454, loss = 1.5508616376313862
learning rate: 0.0017229535728773408
step:300
train performance: acc = 0.4492753623188406, loss = 1.5301836937181172
test performance: acc = 0.451, loss = 1.5421372375690425
learning rate: 0.0017229535728773408
step:400
train performance: acc = 0.5369946605644547, loss = 1.3728291541509543
test performance: acc = 0.49, loss = 1.460830536422421
learning rate: 0.0017229535728773408
step:500
train performance: acc = 0.5369946605644547, loss = 1.3144872346054541
test performance: acc = 0.4862, loss = 1.477585483457378
learning rate: 0.0017229535728773408
step:600
train performance: acc = 0.5118230358504958, loss = 1.3908141311884041
test performance: acc = 0.4798, loss = 1.4991478173578008
learning rate: 0.0017229535728773408
step:700
train performance: acc = 0.555301296720061, loss = 1.2965905451313224
test performance: acc = 0.4975, loss = 1.4170973707309198
learning rate: 0.0017229535728773408
step:800
train performance: acc = 0.5186880244088482, loss = 1.4049671730352322
test performance: acc = 0.4685, loss = 1.5639763374687983
learning rate: 0.0017229535728773408
step:900
train performance: acc = 0.540045766590389, loss = 1.3841871942334643
test performance: acc = 0.4789, loss = 1.5115931439099721
learning rate: 0.0017229535728773408
step:1000
train performance: acc = 0.5766590389016019, loss = 1.2139594174176678
test performance: acc = 0.4902, loss = 1.4600020699618022
learning rate: 0.0017229535728773408
step:1100
train performance: acc = 0.5347063310450039, loss = 1.2878160609662281
test performance: acc = 0.483, loss = 1.4674643109902192
learning rate: 0.0017229535728773408
step:1200
train performance: acc = 0.5850495804729214, loss = 1.2297006437936595
test performance: acc = 0.4966, loss = 1.4417486110024476
learning rate: 0.0017229535728773408
step:1300
train performance: acc = 0.559115179252479, loss = 1.351051425207613
test performance: acc = 0.4648, loss = 1.615970805216586
learning rate: 0.0017229535728773408
step:1400
train performance: acc = 0.5919145690312738, loss = 1.1776071512895177
test performance: acc = 0.4995, loss = 1.4516194288198163
learning rate: 0.0017229535728773408
step:1500
train performance: acc = 0.5949656750572082, loss = 1.1544570734323518
test performance: acc = 0.5036, loss = 1.4437477267748002
learning rate: 0.0017229535728773408
step:1600
train performance: acc = 0.5339435545385202, loss = 1.442735214003213
test performance: acc = 0.4601, loss = 1.6463799897353761
learning rate: 0.0017229535728773408
step:1700
train performance: acc = 0.559115179252479, loss = 1.31885862439577
test performance: acc = 0.4869, loss = 1.5661676498887163
learning rate: 0.0017229535728773408
step:1800
train performance: acc = 0.6125095347063311, loss = 1.1550373108234027
test performance: acc = 0.5003, loss = 1.4518404228175765
learning rate: 0.0017229535728773408
step:1900
train performance: acc = 0.6209000762776506, loss = 1.160185276853648
test performance: acc = 0.4937, loss = 1.4807645279909856
learning rate: 0.0017229535728773408
step:2000
train performance: acc = 0.5926773455377574, loss = 1.1700468095431322
test performance: acc = 0.494, loss = 1.4782659881951508
Early Stop
saving results in folder...
saving model in folder
ind 9 epoc 1
dnn_hidden_units : 10,10,10,10,10
learning_rate : 0.000921214367156
max_steps : 2217
batch_size : 1123
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0009212143671557149
step:0
train performance: acc = 0.10151380231522707, loss = 2.302584992994049
test performance: acc = 0.1, loss = 2.302584993015523
learning rate: 0.0009212143671557149
step:100
train performance: acc = 0.09706144256455922, loss = 2.3025862072162537
test performance: acc = 0.1, loss = 2.3025849935891514
learning rate: 0.0009212143671557149
step:200
train performance: acc = 0.10151380231522707, loss = 2.302584337802526
test performance: acc = 0.1, loss = 2.302584993478752
learning rate: 0.0009212143671557149
step:300
train performance: acc = 0.09617097061442564, loss = 2.302585980505161
test performance: acc = 0.1, loss = 2.302584993451935
learning rate: 0.0009212143671557149
step:400
train performance: acc = 0.10685663401602849, loss = 2.302584112949513
test performance: acc = 0.1, loss = 2.302584993310658
learning rate: 0.0009212143671557149
step:500
train performance: acc = 0.10151380231522707, loss = 2.302586535352831
test performance: acc = 0.1, loss = 2.3025849933712785
learning rate: 0.0009212143671557149
step:600
train performance: acc = 0.09884238646482636, loss = 2.3025858644789694
test performance: acc = 0.1, loss = 2.302584993261498
learning rate: 0.0009212143671557149
step:700
train performance: acc = 0.08726625111308994, loss = 2.3025867973007093
test performance: acc = 0.1, loss = 2.3025849933074425
learning rate: 0.0009212143671557149
step:800
train performance: acc = 0.10151380231522707, loss = 2.3025854763432316
test performance: acc = 0.1, loss = 2.3025849934478546
learning rate: 0.0009212143671557149
step:900
train performance: acc = 0.10240427426536064, loss = 2.302586163586225
test performance: acc = 0.1, loss = 2.302584993512847
learning rate: 0.0009212143671557149
step:1000
train performance: acc = 0.09706144256455922, loss = 2.302587680753145
test performance: acc = 0.1, loss = 2.3025849935256977
Early Stop
saving results in folder...
saving model in folder
accuracies: [0.27850000000000003, 0.1013, 0.32529999999999998, 0.49919999999999998, 0.11899999999999999, 0.379, 0.43149999999999999, 0.49399999999999999, 0.10000000000000001]
ind 0 epoc 2
dnn_hidden_units : 200
learning_rate : 0.00118867479594
max_steps : 2881
batch_size : 1671
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0011886747959448368
step:0
train performance: acc = 0.09096349491322561, loss = 2.3032629839863477
test performance: acc = 0.1703, loss = 2.2924117262203016
learning rate: 0.0011886747959448368
step:100
train performance: acc = 0.4135248354278875, loss = 1.6953563138516567
test performance: acc = 0.4019, loss = 1.700436356321651
learning rate: 0.0011886747959448368
step:200
train performance: acc = 0.44703770197486536, loss = 1.5663584091961145
test performance: acc = 0.4511, loss = 1.572512497094998
learning rate: 0.0011886747959448368
step:300
train performance: acc = 0.4823459006582885, loss = 1.4970391377340642
test performance: acc = 0.4756, loss = 1.5021163511072049
learning rate: 0.0011886747959448368
step:400
train performance: acc = 0.5278276481149012, loss = 1.3815426110597537
test performance: acc = 0.4894, loss = 1.4600732729073649
learning rate: 0.0011886747959448368
step:500
train performance: acc = 0.5224416517055656, loss = 1.3713410525173284
test performance: acc = 0.4993, loss = 1.4267862021027955
learning rate: 0.0011886747959448368
step:600
train performance: acc = 0.5152603231597845, loss = 1.3630221469888832
test performance: acc = 0.4926, loss = 1.4422506361002319
learning rate: 0.0011886747959448368
step:700
train performance: acc = 0.5493716337522442, loss = 1.3107986010436588
test performance: acc = 0.4988, loss = 1.4391732583703452
learning rate: 0.0011886747959448368
step:800
train performance: acc = 0.5709156193895871, loss = 1.2417385847345375
test performance: acc = 0.5133, loss = 1.380820898396066
learning rate: 0.0011886747959448368
step:900
train performance: acc = 0.5673249551166966, loss = 1.250738242407354
test performance: acc = 0.5229, loss = 1.3778899784106278
learning rate: 0.0011886747959448368
step:1000
train performance: acc = 0.5559545182525434, loss = 1.3229639932570467
test performance: acc = 0.498, loss = 1.4578749564909306
learning rate: 0.0011886747959448368
step:1100
train performance: acc = 0.5709156193895871, loss = 1.2549538422916495
test performance: acc = 0.5052, loss = 1.4464357906797125
learning rate: 0.0011886747959448368
step:1200
train performance: acc = 0.6056253740275285, loss = 1.135689197040491
test performance: acc = 0.5144, loss = 1.3803477264768274
learning rate: 0.0011886747959448368
step:1300
train performance: acc = 0.6146020347097546, loss = 1.131602825905777
test performance: acc = 0.5158, loss = 1.3838726274487736
learning rate: 0.0011886747959448368
step:1400
train performance: acc = 0.594853381208857, loss = 1.2003164649070985
test performance: acc = 0.4965, loss = 1.4810216978879758
learning rate: 0.0011886747959448368
step:1500
train performance: acc = 0.5966487133453022, loss = 1.177223887233662
test performance: acc = 0.4964, loss = 1.4642303223644013
learning rate: 0.0011886747959448368
step:1600
train performance: acc = 0.6265709156193896, loss = 1.0919846598823741
test performance: acc = 0.5186, loss = 1.377981772264059
learning rate: 0.0011886747959448368
step:1700
train performance: acc = 0.6128067025733094, loss = 1.1130831012837108
test performance: acc = 0.5229, loss = 1.389685029551084
learning rate: 0.0011886747959448368
step:1800
train performance: acc = 0.6457211250748055, loss = 1.034979613086599
test performance: acc = 0.5262, loss = 1.373231592651649
learning rate: 0.0011886747959448368
step:1900
train performance: acc = 0.642130460801915, loss = 1.0399070458158877
test performance: acc = 0.5096, loss = 1.4371079632281645
learning rate: 0.0011886747959448368
step:2000
train performance: acc = 0.5631358467983244, loss = 1.3830064112462712
test performance: acc = 0.474, loss = 1.663492060824055
learning rate: 0.0011886747959448368
step:2100
train performance: acc = 0.5739078396169958, loss = 1.3380996423128506
test performance: acc = 0.489, loss = 1.5497910917745779
Early Stop
saving results in folder...
saving model in folder
ind 1 epoc 2
dnn_hidden_units : 50,10
learning_rate : 0.00216212859058
max_steps : 3693
batch_size : 1870
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0021621285905794813
step:0
train performance: acc = 0.10267379679144385, loss = 2.3025890096190866
test performance: acc = 0.1012, loss = 2.302587951939611
learning rate: 0.0021621285905794813
step:100
train performance: acc = 0.1267379679144385, loss = 2.3025844704451637
test performance: acc = 0.1261, loss = 2.3025832700920583
learning rate: 0.0021621285905794813
step:200
train performance: acc = 0.09786096256684491, loss = 2.3025767022377233
test performance: acc = 0.0846, loss = 2.302576969152435
learning rate: 0.0021621285905794813
step:300
train performance: acc = 0.16844919786096257, loss = 2.302566390586007
test performance: acc = 0.1336, loss = 2.3025678500312003
learning rate: 0.0021621285905794813
step:400
train performance: acc = 0.10481283422459893, loss = 2.3025491790643886
test performance: acc = 0.1183, loss = 2.3025528306362113
learning rate: 0.0021621285905794813
step:500
train performance: acc = 0.1716577540106952, loss = 2.3025218570301207
test performance: acc = 0.1673, loss = 2.3025236325088017
learning rate: 0.0021621285905794813
step:600
train performance: acc = 0.1588235294117647, loss = 2.3024446349147425
test performance: acc = 0.1551, loss = 2.3024537027924485
learning rate: 0.0021621285905794813
step:700
train performance: acc = 0.15614973262032086, loss = 2.3022006090055944
test performance: acc = 0.1503, loss = 2.3022281744108066
learning rate: 0.0021621285905794813
step:800
train performance: acc = 0.1572192513368984, loss = 2.301071631389778
test performance: acc = 0.1529, loss = 2.300934338822629
learning rate: 0.0021621285905794813
step:900
train performance: acc = 0.1657754010695187, loss = 2.274128980128693
test performance: acc = 0.1624, loss = 2.271839627938859
learning rate: 0.0021621285905794813
step:1000
train performance: acc = 0.16417112299465242, loss = 2.177461812344477
test performance: acc = 0.1734, loss = 2.1859134988838456
learning rate: 0.0021621285905794813
step:1100
train performance: acc = 0.16524064171122996, loss = 2.1813504247764377
test performance: acc = 0.1789, loss = 2.170687326429453
learning rate: 0.0021621285905794813
step:1200
train performance: acc = 0.20053475935828877, loss = 2.145010834361517
test performance: acc = 0.1803, loss = 2.1646024272461935
learning rate: 0.0021621285905794813
step:1300
train performance: acc = 0.17700534759358288, loss = 2.1512720570217594
test performance: acc = 0.1804, loss = 2.1591013899320117
learning rate: 0.0021621285905794813
step:1400
train performance: acc = 0.18288770053475936, loss = 2.15713203429578
test performance: acc = 0.1818, loss = 2.1531106441692835
learning rate: 0.0021621285905794813
step:1500
train performance: acc = 0.19679144385026737, loss = 2.1513279736331996
test performance: acc = 0.1826, loss = 2.139763886962662
learning rate: 0.0021621285905794813
step:1600
train performance: acc = 0.19144385026737967, loss = 2.0669977074898993
test performance: acc = 0.1939, loss = 2.0699123384059828
learning rate: 0.0021621285905794813
step:1700
train performance: acc = 0.19946524064171123, loss = 2.0154809468530015
test performance: acc = 0.199, loss = 2.034265766832442
learning rate: 0.0021621285905794813
step:1800
train performance: acc = 0.21176470588235294, loss = 2.0143103707274195
test performance: acc = 0.2048, loss = 2.005724029524428
learning rate: 0.0021621285905794813
step:1900
train performance: acc = 0.21390374331550802, loss = 2.002281305699269
test performance: acc = 0.2164, loss = 1.983230548499994
learning rate: 0.0021621285905794813
step:2000
train performance: acc = 0.22780748663101605, loss = 1.9576857154193819
test performance: acc = 0.2403, loss = 1.9519832310097627
learning rate: 0.0021621285905794813
step:2100
train performance: acc = 0.26737967914438504, loss = 1.8514908097778358
test performance: acc = 0.2643, loss = 1.8946048739919115
learning rate: 0.0021621285905794813
step:2200
train performance: acc = 0.3085561497326203, loss = 1.8436554608272429
test performance: acc = 0.2968, loss = 1.8563043776557562
learning rate: 0.0021621285905794813
step:2300
train performance: acc = 0.30213903743315507, loss = 1.8467311043467827
test performance: acc = 0.3132, loss = 1.821302852821887
learning rate: 0.0021621285905794813
step:2400
train performance: acc = 0.31871657754010696, loss = 1.7964585497223449
test performance: acc = 0.3374, loss = 1.7849845996720342
learning rate: 0.0021621285905794813
step:2500
train performance: acc = 0.3518716577540107, loss = 1.7503243751589048
test performance: acc = 0.3502, loss = 1.753292337058506
learning rate: 0.0021621285905794813
step:2600
train performance: acc = 0.34705882352941175, loss = 1.736224212720048
test performance: acc = 0.3653, loss = 1.7234392984278646
learning rate: 0.0021621285905794813
step:2700
train performance: acc = 0.3866310160427808, loss = 1.662686972027368
test performance: acc = 0.3828, loss = 1.6953957507166677
learning rate: 0.0021621285905794813
step:2800
train performance: acc = 0.3893048128342246, loss = 1.690609642108173
test performance: acc = 0.3925, loss = 1.668127089517165
learning rate: 0.0021621285905794813
step:2900
train performance: acc = 0.3967914438502674, loss = 1.6549610019642207
test performance: acc = 0.4006, loss = 1.6414812520794022
learning rate: 0.0021621285905794813
step:3000
train performance: acc = 0.41336898395721927, loss = 1.5887819481864485
test performance: acc = 0.4153, loss = 1.6180510532758183
learning rate: 0.0021621285905794813
step:3100
train performance: acc = 0.4283422459893048, loss = 1.600255010087156
test performance: acc = 0.4259, loss = 1.5945607347064552
learning rate: 0.0021621285905794813
step:3200
train performance: acc = 0.4358288770053476, loss = 1.5541673011740058
test performance: acc = 0.4293, loss = 1.5951727950650487
learning rate: 0.0021621285905794813
step:3300
train performance: acc = 0.4406417112299465, loss = 1.5513193690064835
test performance: acc = 0.4371, loss = 1.5687942656273375
learning rate: 0.0021621285905794813
step:3400
train performance: acc = 0.46310160427807484, loss = 1.5045876434627752
test performance: acc = 0.4463, loss = 1.5362191158981129
learning rate: 0.0021621285905794813
step:3500
train performance: acc = 0.45294117647058824, loss = 1.49871360520319
test performance: acc = 0.4532, loss = 1.5223564205342397
learning rate: 0.0021621285905794813
step:3600
train performance: acc = 0.4919786096256685, loss = 1.4566560203336198
test performance: acc = 0.4568, loss = 1.518455596247911
learning rate: 0.0021621285905794813
step:3692
train performance: acc = 0.5117647058823529, loss = 1.4150452306491224
test performance: acc = 0.462, loss = 1.504276304391538
saving results in folder...
saving model in folder
ind 2 epoc 2
dnn_hidden_units : 100,10
learning_rate : 0.0133466857833
max_steps : 2718
batch_size : 1135
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.013346685783340329
step:0
train performance: acc = 0.10484581497797357, loss = 2.3025841453280136
test performance: acc = 0.1022, loss = 2.302582247579107
learning rate: 0.013346685783340329
step:100
train performance: acc = 0.12951541850220263, loss = 2.2987796581866475
test performance: acc = 0.1349, loss = 2.2981835417503187
learning rate: 0.013346685783340329
step:200
train performance: acc = 0.186784140969163, loss = 2.076709223816552
test performance: acc = 0.2017, loss = 2.071794004903279
learning rate: 0.013346685783340329
step:300
train performance: acc = 0.3162995594713656, loss = 1.836885486370107
test performance: acc = 0.3068, loss = 1.8500636381255329
learning rate: 0.013346685783340329
step:400
train performance: acc = 0.37180616740088107, loss = 1.7590518577837724
test performance: acc = 0.3746, loss = 1.7193778125560903
learning rate: 0.013346685783340329
step:500
train performance: acc = 0.4167400881057269, loss = 1.6388952665583476
test performance: acc = 0.4222, loss = 1.6298660154664537
learning rate: 0.013346685783340329
step:600
train performance: acc = 0.3964757709251101, loss = 1.7500485056043735
test performance: acc = 0.4184, loss = 1.6417788675230611
learning rate: 0.013346685783340329
step:700
train performance: acc = 0.4079295154185022, loss = 1.739900263657802
test performance: acc = 0.4474, loss = 1.5755046289602934
learning rate: 0.013346685783340329
step:800
train performance: acc = 0.46519823788546255, loss = 1.479939294121054
test performance: acc = 0.4712, loss = 1.5019776869927801
learning rate: 0.013346685783340329
step:900
train performance: acc = 0.4977973568281938, loss = 1.4174180166238024
test performance: acc = 0.4832, loss = 1.47595829612876
learning rate: 0.013346685783340329
step:1000
train performance: acc = 0.4889867841409692, loss = 1.3955776846739019
test performance: acc = 0.4555, loss = 1.5531012235798716
learning rate: 0.013346685783340329
step:1100
train performance: acc = 0.5312775330396475, loss = 1.3505402843801642
test performance: acc = 0.4855, loss = 1.4627695300273167
learning rate: 0.013346685783340329
step:1200
train performance: acc = 0.5004405286343613, loss = 1.403625011353392
test performance: acc = 0.4884, loss = 1.4525129480287224
learning rate: 0.013346685783340329
step:1300
train performance: acc = 0.4907488986784141, loss = 1.4143554729500845
test performance: acc = 0.4621, loss = 1.5452325887434377
learning rate: 0.013346685783340329
step:1400
train performance: acc = 0.5277533039647577, loss = 1.3233170626453694
test performance: acc = 0.4851, loss = 1.464503898079657
learning rate: 0.013346685783340329
step:1500
train performance: acc = 0.5691629955947136, loss = 1.2588598565467188
test performance: acc = 0.4878, loss = 1.4945231085261486
learning rate: 0.013346685783340329
step:1600
train performance: acc = 0.5162995594713656, loss = 1.3517434977791905
test performance: acc = 0.4792, loss = 1.5304325389847317
learning rate: 0.013346685783340329
step:1700
train performance: acc = 0.5859030837004405, loss = 1.1825226394084423
test performance: acc = 0.4893, loss = 1.475472065038768
learning rate: 0.013346685783340329
step:1800
train performance: acc = 0.5224669603524229, loss = 1.3143446435587534
test performance: acc = 0.4662, loss = 1.544943005895345
learning rate: 0.013346685783340329
step:1900
train performance: acc = 0.5682819383259912, loss = 1.2270912422875895
test performance: acc = 0.491, loss = 1.4749000562538492
learning rate: 0.013346685783340329
step:2000
train performance: acc = 0.5180616740088105, loss = 1.3919555226812674
test performance: acc = 0.4635, loss = 1.575012926917333
learning rate: 0.013346685783340329
step:2100
train performance: acc = 0.5365638766519824, loss = 1.281431839974875
test performance: acc = 0.477, loss = 1.5267161929029793
learning rate: 0.013346685783340329
step:2200
train performance: acc = 0.5656387665198238, loss = 1.247936729631189
test performance: acc = 0.4567, loss = 1.653836508171545
learning rate: 0.013346685783340329
step:2300
train performance: acc = 0.5568281938325991, loss = 1.2637211680338891
test performance: acc = 0.4615, loss = 1.5747279259690456
learning rate: 0.013346685783340329
step:2400
train performance: acc = 0.6026431718061674, loss = 1.1508403143361936
test performance: acc = 0.5017, loss = 1.4762835563116719
Early Stop
saving results in folder...
saving model in folder
ind 4 epoc 2
dnn_hidden_units : 300
learning_rate : 1e-05
max_steps : 2618
batch_size : 1810
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 1e-05
step:0
train performance: acc = 0.08397790055248619, loss = 2.3077692054448997
test performance: acc = 0.0822, loss = 2.307515678788568
learning rate: 1e-05
step:100
train performance: acc = 0.13646408839779006, loss = 2.2950726126170125
test performance: acc = 0.1343, loss = 2.293371026778347
learning rate: 1e-05
step:200
train performance: acc = 0.18121546961325966, loss = 2.2809784214679523
test performance: acc = 0.1945, loss = 2.279678503553605
learning rate: 1e-05
step:300
train performance: acc = 0.2165745856353591, loss = 2.265498968119173
test performance: acc = 0.2115, loss = 2.2659353449903374
learning rate: 1e-05
step:400
train performance: acc = 0.2121546961325967, loss = 2.2537169475776517
test performance: acc = 0.2238, loss = 2.2518555337461867
learning rate: 1e-05
step:500
train performance: acc = 0.2143646408839779, loss = 2.240960281213051
test performance: acc = 0.2315, loss = 2.237254862074376
learning rate: 1e-05
step:600
train performance: acc = 0.2303867403314917, loss = 2.222482984337648
test performance: acc = 0.2373, loss = 2.2223076816747738
learning rate: 1e-05
step:700
train performance: acc = 0.2281767955801105, loss = 2.210253364052626
test performance: acc = 0.2412, loss = 2.207021948758674
learning rate: 1e-05
step:800
train performance: acc = 0.24861878453038674, loss = 2.1893561622495925
test performance: acc = 0.246, loss = 2.1917175524898544
learning rate: 1e-05
step:900
train performance: acc = 0.23591160220994475, loss = 2.1839059844210444
test performance: acc = 0.2506, loss = 2.1766870936047784
learning rate: 1e-05
step:1000
train performance: acc = 0.2430939226519337, loss = 2.169399119968319
test performance: acc = 0.2546, loss = 2.162063966786266
learning rate: 1e-05
step:1100
train performance: acc = 0.23370165745856353, loss = 2.1583774116477024
test performance: acc = 0.2596, loss = 2.1478833174350074
learning rate: 1e-05
step:1200
train performance: acc = 0.25248618784530386, loss = 2.1467414721108384
test performance: acc = 0.263, loss = 2.1343644390288175
learning rate: 1e-05
step:1300
train performance: acc = 0.26519337016574585, loss = 2.1136700437021543
test performance: acc = 0.2673, loss = 2.1213364472006355
learning rate: 1e-05
step:1400
train performance: acc = 0.26022099447513813, loss = 2.1069000447250668
test performance: acc = 0.2683, loss = 2.108831799932663
learning rate: 1e-05
step:1500
train performance: acc = 0.26795580110497236, loss = 2.1063912746288103
test performance: acc = 0.271, loss = 2.096783545345457
learning rate: 1e-05
step:1600
train performance: acc = 0.2718232044198895, loss = 2.0851528990918573
test performance: acc = 0.2756, loss = 2.085188327331845
learning rate: 1e-05
step:1700
train performance: acc = 0.2861878453038674, loss = 2.059475454527056
test performance: acc = 0.2786, loss = 2.0740616137759913
learning rate: 1e-05
step:1800
train performance: acc = 0.2823204419889503, loss = 2.0718483070371008
test performance: acc = 0.2811, loss = 2.0633710778634304
learning rate: 1e-05
step:1900
train performance: acc = 0.2856353591160221, loss = 2.0431458267993947
test performance: acc = 0.2841, loss = 2.053066571555401
learning rate: 1e-05
step:2000
train performance: acc = 0.2773480662983425, loss = 2.0561638907723316
test performance: acc = 0.2858, loss = 2.0431561250584447
learning rate: 1e-05
step:2100
train performance: acc = 0.2839779005524862, loss = 2.0423533432597685
test performance: acc = 0.2886, loss = 2.033675396598462
learning rate: 1e-05
step:2200
train performance: acc = 0.2955801104972376, loss = 2.0323501760450258
test performance: acc = 0.29, loss = 2.0244753927053862
learning rate: 1e-05
step:2300
train performance: acc = 0.28342541436464086, loss = 2.016308944032709
test performance: acc = 0.292, loss = 2.0157170762224266
learning rate: 1e-05
step:2400
train performance: acc = 0.2856353591160221, loss = 2.010363711235177
test performance: acc = 0.2949, loss = 2.0072581289009004
learning rate: 1e-05
step:2500
train performance: acc = 0.28342541436464086, loss = 2.011380509956872
test performance: acc = 0.2968, loss = 1.9991404698702568
learning rate: 1e-05
step:2600
train performance: acc = 0.292817679558011, loss = 2.0174046658239324
test performance: acc = 0.2983, loss = 1.9913439400142554
learning rate: 1e-05
step:2617
train performance: acc = 0.2988950276243094, loss = 1.9827088471882908
test performance: acc = 0.2986, loss = 1.990011891511659
saving results in folder...
saving model in folder
ind 5 epoc 2
dnn_hidden_units : 100
learning_rate : 0.00896249134189
max_steps : 4194
batch_size : 1485
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.008962491341889844
step:0
train performance: acc = 0.08080808080808081, loss = 2.3035387519927
test performance: acc = 0.2209, loss = 2.258033205228169
learning rate: 0.008962491341889844
step:100
train performance: acc = 0.25117845117845117, loss = 8.668262287242456
test performance: acc = 0.2202, loss = 8.489909268185814
learning rate: 0.008962491341889844
step:200
train performance: acc = 0.2814814814814815, loss = 8.441743137052134
test performance: acc = 0.225, loss = 8.66395502299112
learning rate: 0.008962491341889844
step:300
train performance: acc = 0.2868686868686869, loss = 7.732989828098353
test performance: acc = 0.258, loss = 8.429451089065711
learning rate: 0.008962491341889844
step:400
train performance: acc = 0.19865319865319866, loss = 9.650399156824632
test performance: acc = 0.2198, loss = 8.90409266460195
learning rate: 0.008962491341889844
step:500
train performance: acc = 0.3151515151515151, loss = 7.139814826392644
test performance: acc = 0.2867, loss = 7.422509394588906
learning rate: 0.008962491341889844
step:600
train performance: acc = 0.3023569023569024, loss = 7.408207119792466
test performance: acc = 0.2751, loss = 7.647995464722272
learning rate: 0.008962491341889844
step:700
train performance: acc = 0.296969696969697, loss = 7.459359982844596
test performance: acc = 0.2987, loss = 7.254226066426661
learning rate: 0.008962491341889844
step:800
train performance: acc = 0.32727272727272727, loss = 7.1178545712577534
test performance: acc = 0.2616, loss = 8.488819602574022
learning rate: 0.008962491341889844
step:900
train performance: acc = 0.28215488215488216, loss = 8.203082118773198
test performance: acc = 0.2498, loss = 9.349909083564677
learning rate: 0.008962491341889844
step:1000
train performance: acc = 0.27205387205387205, loss = 8.294487068647998
test performance: acc = 0.2694, loss = 7.803487116053919
learning rate: 0.008962491341889844
step:1100
train performance: acc = 0.3063973063973064, loss = 6.987507973353666
test performance: acc = 0.3361, loss = 6.405355019265519
learning rate: 0.008962491341889844
step:1200
train performance: acc = 0.3191919191919192, loss = 7.579403688060405
test performance: acc = 0.2878, loss = 7.217813500307862
learning rate: 0.008962491341889844
step:1300
train performance: acc = 0.336026936026936, loss = 6.643280305288554
test performance: acc = 0.2854, loss = 7.501652503202834
learning rate: 0.008962491341889844
step:1400
train performance: acc = 0.3400673400673401, loss = 6.837082247645053
test performance: acc = 0.3447, loss = 6.43663952672482
learning rate: 0.008962491341889844
step:1500
train performance: acc = 0.35353535353535354, loss = 6.970569267067558
test performance: acc = 0.3225, loss = 6.833782863745606
learning rate: 0.008962491341889844
step:1600
train performance: acc = 0.3824915824915825, loss = 5.768849721121782
test performance: acc = 0.3072, loss = 7.083410445906414
learning rate: 0.008962491341889844
step:1700
train performance: acc = 0.36835016835016837, loss = 6.621460282085807
test performance: acc = 0.3202, loss = 7.576861217667168
learning rate: 0.008962491341889844
step:1800
train performance: acc = 0.3616161616161616, loss = 7.114822653105042
test performance: acc = 0.319, loss = 7.370275975851924
learning rate: 0.008962491341889844
step:1900
train performance: acc = 0.33400673400673403, loss = 7.000980558148671
test performance: acc = 0.3255, loss = 7.065998181646889
learning rate: 0.008962491341889844
step:2000
train performance: acc = 0.34545454545454546, loss = 7.38866120543246
test performance: acc = 0.3256, loss = 7.2214404393491325
learning rate: 0.008962491341889844
step:2100
train performance: acc = 0.3643097643097643, loss = 6.926694822478483
test performance: acc = 0.3433, loss = 7.486424819906807
Early Stop
saving results in folder...
saving model in folder
ind 6 epoc 2
dnn_hidden_units : 100
learning_rate : 0.00169898569663
max_steps : 2738
batch_size : 837
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0016989856966318933
step:0
train performance: acc = 0.07765830346475508, loss = 2.304224292256014
test performance: acc = 0.1695, loss = 2.294084936294184
learning rate: 0.0016989856966318933
step:100
train performance: acc = 0.4074074074074074, loss = 1.653216682057342
test performance: acc = 0.4126, loss = 1.6666110803308767
learning rate: 0.0016989856966318933
step:200
train performance: acc = 0.48148148148148145, loss = 1.5547161326950267
test performance: acc = 0.4536, loss = 1.5525429964915791
learning rate: 0.0016989856966318933
step:300
train performance: acc = 0.4958183990442055, loss = 1.4669911470911605
test performance: acc = 0.4698, loss = 1.503603204057218
learning rate: 0.0016989856966318933
step:400
train performance: acc = 0.5113500597371565, loss = 1.4360110404309572
test performance: acc = 0.4763, loss = 1.4665881133845662
learning rate: 0.0016989856966318933
step:500
train performance: acc = 0.5256869772998806, loss = 1.3676078924835515
test performance: acc = 0.4851, loss = 1.461321264529506
learning rate: 0.0016989856966318933
step:600
train performance: acc = 0.5280764635603346, loss = 1.4377943163120515
test performance: acc = 0.484, loss = 1.4698113724800517
learning rate: 0.0016989856966318933
step:700
train performance: acc = 0.5161290322580645, loss = 1.3805482226436105
test performance: acc = 0.4939, loss = 1.4495421144843792
learning rate: 0.0016989856966318933
step:800
train performance: acc = 0.5639187574671446, loss = 1.2999165026157227
test performance: acc = 0.4884, loss = 1.4917270022731175
learning rate: 0.0016989856966318933
step:900
train performance: acc = 0.5483870967741935, loss = 1.3111553045113982
test performance: acc = 0.4915, loss = 1.4497457579821793
learning rate: 0.0016989856966318933
step:1000
train performance: acc = 0.5746714456391876, loss = 1.2710270171658482
test performance: acc = 0.4819, loss = 1.463545383306131
learning rate: 0.0016989856966318933
step:1100
train performance: acc = 0.5113500597371565, loss = 1.4509499436671238
test performance: acc = 0.4695, loss = 1.5149326325955212
learning rate: 0.0016989856966318933
step:1200
train performance: acc = 0.5806451612903226, loss = 1.2191857344938988
test performance: acc = 0.5091, loss = 1.4037505608713345
learning rate: 0.0016989856966318933
step:1300
train performance: acc = 0.6009557945041816, loss = 1.217901326472824
test performance: acc = 0.5043, loss = 1.42419493950841
learning rate: 0.0016989856966318933
step:1400
train performance: acc = 0.5782556750298686, loss = 1.2875930818532542
test performance: acc = 0.4778, loss = 1.520092861054324
learning rate: 0.0016989856966318933
step:1500
train performance: acc = 0.5758661887694145, loss = 1.186987624904478
test performance: acc = 0.5014, loss = 1.4325245975945384
learning rate: 0.0016989856966318933
step:1600
train performance: acc = 0.6308243727598566, loss = 1.0961846228757606
test performance: acc = 0.5034, loss = 1.4277355757741115
learning rate: 0.0016989856966318933
step:1700
train performance: acc = 0.5686977299880526, loss = 1.22327747547257
test performance: acc = 0.4823, loss = 1.569040153565917
learning rate: 0.0016989856966318933
step:1800
train performance: acc = 0.5925925925925926, loss = 1.2129327078708942
test performance: acc = 0.4965, loss = 1.4877441132670728
learning rate: 0.0016989856966318933
step:1900
train performance: acc = 0.5961768219832736, loss = 1.1741011390941976
test performance: acc = 0.4941, loss = 1.488915439078735
learning rate: 0.0016989856966318933
step:2000
train performance: acc = 0.5710872162485066, loss = 1.2315130971247696
test performance: acc = 0.4831, loss = 1.5136270871420587
learning rate: 0.0016989856966318933
step:2100
train performance: acc = 0.6117084826762246, loss = 1.1092363079940704
test performance: acc = 0.4905, loss = 1.5065190527615144
learning rate: 0.0016989856966318933
step:2200
train performance: acc = 0.6200716845878136, loss = 1.138580845631609
test performance: acc = 0.5053, loss = 1.4661531240451244
learning rate: 0.0016989856966318933
step:2300
train performance: acc = 0.6021505376344086, loss = 1.1530413084848854
test performance: acc = 0.4846, loss = 1.5347370785853154
learning rate: 0.0016989856966318933
step:2400
train performance: acc = 0.6140979689366786, loss = 1.0798980214381786
test performance: acc = 0.5168, loss = 1.419438930379742
learning rate: 0.0016989856966318933
step:2500
train performance: acc = 0.6248506571087217, loss = 1.1270988560689792
test performance: acc = 0.4984, loss = 1.483962023308361
learning rate: 0.0016989856966318933
step:2600
train performance: acc = 0.6391875746714456, loss = 1.0685057965567974
test performance: acc = 0.4963, loss = 1.4836214358637565
learning rate: 0.0016989856966318933
step:2700
train performance: acc = 0.6200716845878136, loss = 1.1465164250681015
test performance: acc = 0.49, loss = 1.5123834621861212
learning rate: 0.0016989856966318933
step:2737
train performance: acc = 0.6069295101553166, loss = 1.113077508695671
test performance: acc = 0.492, loss = 1.5242321533601066
saving results in folder...
saving model in folder
ind 7 epoc 2
dnn_hidden_units : 200
learning_rate : 0.001734208588
max_steps : 2199
batch_size : 1708
eval_freq : 100
data_dir : ./cifar10/cifar-10-batches-py
learning rate: 0.0017342085879971593
step:0
train performance: acc = 0.09133489461358314, loss = 2.3031831678486854
test performance: acc = 0.1925, loss = 2.2866583376390923
learning rate: 0.0017342085879971593
step:100
train performance: acc = 0.4338407494145199, loss = 1.6382950806711738
test performance: acc = 0.4273, loss = 1.6342077072097652
learning rate: 0.0017342085879971593
step:200
train performance: acc = 0.47599531615925056, loss = 1.506534733873042
test performance: acc = 0.4683, loss = 1.5152247188842547
learning rate: 0.0017342085879971593
step:300
train performance: acc = 0.4882903981264637, loss = 1.4548712222059645
test performance: acc = 0.4801, loss = 1.4729669106215255
learning rate: 0.0017342085879971593
step:400
train performance: acc = 0.5128805620608899, loss = 1.3895162172761462
test performance: acc = 0.4843, loss = 1.4530734958697593
learning rate: 0.0017342085879971593
step:500
train performance: acc = 0.5181498829039812, loss = 1.4026180944399766
test performance: acc = 0.4833, loss = 1.4897150445541543
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** JOB 2105919 ON r30n6 CANCELLED AT 2019-04-12T11:25:40 ***
slurmstepd: error: *** STEP 2105919.0 ON r30n6 CANCELLED AT 2019-04-12T11:25:40 ***
